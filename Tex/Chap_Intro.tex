\chapter{引言} \label{chap:introduction}
在计算资源受限的智能手机~\citep{howard2019searching}、FPGA~\citep{zhang2019skynet}、自动驾驶控制系统~\citep{falcini2017deep} 等终端设备上部署基于深度学习的算法模型已成为目前工业界和学术界的研究热点之一。深度学习通过一系列非线性级联的参数化卷积、矩阵乘法等操作，在大量任务上展现了惊人的威力 \citep{lecun2015deep}；但随着深度学习算法和模型的演进，其对部署设备计算力、存储的需求愈发高昂。终端设备算力、存储空间有限，电池供电的特性更导致了其对频繁访存的能耗开销敏感 \citep{han2017efficient}。因此对模型算力需求、存储需求的控制，是部署深度神经网络至终端设备的主要挑战。

本文旨在通过神经网络量化压缩技术解决上述挑战。深度神经网络一般通过单精度浮点格式（IEEE754 FP32）保存模型参数，并通过部署设备的浮点计算单元进行推理运算；而神经网络量化技术~\citep{qin2020binary} 则通过将模型参数和模型运行时的中间激活映射至相应的 8-bit、4-bit 甚至更低位宽的整数，减少模型存储空间且降低模型运行的复杂度。更低位宽的模型参数缓解了模型存储及运行时访存压力，且相同硬件资源下整数乘法、加法器运算速度高于浮点数值逻辑。目前很多移动端 SoC 包含 DSP、NPU 等专用的整数计算加速模块 \citep{qualcomm2019snapdragon}，进一步提高了设备运行量化神经网络的性能。故神经网络量化压缩技术特别适合基于深度学习的终端模型的部署加速。

神经网络量化压缩技术在很多相对简单的任务和模型上获得了成功~\citep{zhou2016dorefanet, Zhang_2018, li2019additive}；然而，现有工作很少探索该项技术能否应用于其他更复杂的任务和模型——实际部署至终端的算法通常需要在真实世界的数据上执行多种复杂任务，例如目标检测等。目标检测~\citep{zou2019object} 是众多实际落地的算法系统流程的第一环，例如在智能手机人脸解锁流程中，人脸关键点识别、活体识别等模块都依赖于人脸检测算法输出的人脸边界框；在 Google Translate app 的摄像头实时翻译应用中，文字 OCR、自然语言翻译等算法模块也都依赖于文字检测算法的输出结果。因此，探索对于目标检测算法模型的量化压缩技术，有重要的实际应用价值和学术价值，也是本文的研究重点。

在将现有的神经网络量化压缩技术应用至目标检测模型后，我们发现量化后的目标检测模型准确度显著降低，且量化训练难以收敛。通过观察目标检测模型在量化训练中模型参数、激活和各正则化层统计值的分布变化，本文指出目标检测模型参数通道间分布差异较大、量化后模型激活存在较多离群点、BN 等正则化层在量化训练中统计值不稳定是导致目标检测模型量化训练难以收敛的主要原因。根据上述观察，本文对目标检测模型的参数、激活量化函数进行相应改进，并调整了 BN 等正则化层的训练方法，成功地在可接受的检测准确度损失范围额内（4-bit ResNet-18 RetinaNet 检测模型~\citep{He_2016, lin2017focal} 在 MS COCO 数据集~\citep{lin2014microsoft} 上仅下降 0.031 mAP），将 RetinaNet、Faster RCNN~\citep{ren2015faster} 等目标检测模型的参数及激活量化压缩至 4-bit 整数。这一用于目标检测模型的量化压缩技术被称为 Fully Quantized Networks for Object Detection （下文简称 FQN，详见第~\ref{chap::fqn} 章）。

除了针对目标检测模型的量化函数及训练过程的改进外，本文还研究了直接训练量化友好的神经网络模型的通用方法。现有的神经网络量化工作大多使用“预训练——量化感知训练/微调”的流程~\citep{jacob2018quantization, krishnamoorthi2018quantizing, jung2019learning, li2019additive}，即先按照一般神经网络的训练策略在目标数据集上训练得到高准确度的全精度神经网络模型，之后再采用量化感知训练等方法尽量恢复模型在量化部署后的准确度。这样的流程会导致两个主要问题：首先，此类方法为保证模型量化准确度，通常会增加量化感知训练/微调阶段的训练轮数，最终使得量化模型的累计训练开销远高于一般模型；其次是在实际场景中，模型部署阶段不一定能接触到预训练时的所有数据，导致很多方法只能在使用少量训练数据~\citep{he2018learning} 或完全没有训练数据~\citep{nagel2019data, meller2019same} 的情况下恢复量化模型准确度，使得量化模型的最终部署准确度并不理想。如果模型本身在预训练阶段就考虑对于量化部署的友好性，即其参数及激活被量化后仍能保持足够的准确度而不再需要量化感知训练/微调，那么上述问题自然就被解决。

一般神经网络对于量化部署不友好的原因是其参数和激活对量化误差的容忍度低。如果模型训练方法能够在预训练阶段引入模型量化后的数值误差，并设计相应训练策略使得模型在量化误差存在的情况下仍能达到较高准确度，那么就能得到量化友好的、不再需要额外量化训练/微调的神经网络模型。在此思路启发下，本文通过对神经网络模型的量化误差整体建模并将此误差作为正则项加入至预训练过程，最终在原模型相同训练轮数下得到了量化友好模型。在 ImageNet 数据集~\citep{ILSVRC15} 上，使用此方法训练的 4-bit ResNet-18 模型能达到 $67.32\%$ Top-1 准确度，仅比全精度预训练模型准确度下降 $2.57\%$。更重要的是，本文提出的方法无需修改原模型预训练阶段的计算图，且对所用量化函数没有特殊要求，因此具有良好的通用性。这一在预训练阶段引入量化误差从而得到量化友好的神经网络的方法被称为 Guided Quantization Networks （下文简称 GQ-Nets，详见第~\ref{chap::gq_nets} 章）。
% ==============================================================================
%  Motivations
% ==============================================================================
% \section{研究动机}
% TODO
% ==============================================================================
%  Contributions
% ==============================================================================
\section{主要贡献}
\begin{enumerate}
  \item 本文提出了一种适用于目标检测模型的量化压缩算法 FQN。FQN 可在接近原模型准确度的情况下将模型参数和激活端到端量化至 4-bit 整数，且不包含任何浮点操作，使得模型在部署时仅需整数数值逻辑即可完成推理。FQN 在 4-bit 下目标检测准确度相对损失较领域内其他方法低 $3.84\times$（第~\ref{chap::fqn} 章）；
  \item 本文提出了一种在预训练阶段减少模型量化误差，从而得到量化部署友好的神经网络模型的通用算法框架 GQ-Nets。GQ-Nets 能够将通用模型参数及激活量化至 5-bit $\sim$ 2-bit 整数，且无需额外的量化感知训练/微调环节。在完全量化和非完全量化场景下，GQ-Nets 物体分类准确度达到或超过领域内其他方法（第~\ref{chap::gq_nets} 章）；
  \item 本文在 PyTorch~\citep{paszke2019pytorch} 基础上开发了一套用于训练、测试量化神经网络的软件包 \verb|QuantPack|。\verb|QuantPack| 支持不同量化模式和数值精度，可自动完成全精度模型至量化模型的转换和训练，且易于修改和拓展（附录~\ref{chap::quant_pack} ，同时在 GitHub 开源：\url{https://github.com/CrazyRundong/quant-pack}）。
\end{enumerate}