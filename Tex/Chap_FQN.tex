\chapter{用于目标检测的全量化网络} \label{chap::fqn}
% ==============================================================================
%  Introduction
% ==============================================================================
\section{引言}
正如第~\ref{chap::background} 章所介绍，基于深度学习的目标检测技术已在大量真实世界的任务上取得显著成功 \citep{lecun2015deep}。然而，高昂的运算和存储开销，是将目标检测技术广泛应用到诸如智能手机、智能安防相机、自动驾驶系统等计算资源受限场景的主要障碍。为减少深度学习模型的运算和存储开销，近年来学术界和工业界探索了多种解决路径，其中主要方向包括高效的模型算子和结构探索~\citep{howard2017mobilenets, Sandler_2018, iandola2016squeezenet, zhang2018shufflenet}、以部署平台模型推理时间为约束的模型结构自动搜索~\citep{wu2019fbnet, tan2019mnasnet, tan2019efficientnet}、模型剪枝~\citep{han2015learning, li2016pruning, chen2018shallowing} 和模型量化~\citep{rastegari2016xnor, zhou2016dorefanet, jacob2018quantization} 等。其中，模型量化技术由于可以显著减少模型尺寸，同时使用更高效的整数或定点运算逻辑代替一般深度学习模型所需的浮点运算逻辑而提高推理效率，近年来受到了较多关注，也是本章讨论的重点。

现有的模型量化技术在物体分类等相对简单的任务上表现出色~\citep{zhou2016dorefanet, Zhang_2018, li2019additive}，然而将其应用到目标检测~\citep{zou2019object} 这一较为复杂的任务时，会在模型训练及硬件部署时面临额外的挑战：
\begin{enumerate}[1)]
  \item 与只需学习模型输出间偏序关系的分类任务不同，目标检测还需要模型准确地将预设锚框（anchor box）准确地回归至检测物体的边界框（bounding box），低比特量化后的模型可能难以处理这一回归任务；
  \item 检测模型为保持特征图（feature map）的空间信息，其输入图片尺寸较分类任务增大（一般检测模型输入图像短边尺寸为 $600\sim 800$ 像素）而占用更多 GPU 显存，导致训练时单 GPU 批量大小（batch size）缩小至 2 或 4，从而影响模型中部分正则化操作，例如批正则化（Batch Normalization~\citep{ioffe2015batch}，下文简称 BN）层的效率，使得本身就难以训练的量化网络更不稳定、难以收敛；
  \item 大部分运行量化模型的专有硬件，例如终端设备上嵌入的 FPGA 和 DSP 等，为保证硬件利用率和访存效率，并不包含浮点运算单元。这要求量化检测模型在训练后不包含任何浮点操作，而现有的大部分模型量化算法~\citet{zhou2016dorefanet, Zhang_2018, li2019additive} 为保证模型量化后准确度，会将模型的 BN 层等敏感部分数值精度保留为浮点。同时，一些模型量化方法对一般硬件部署并不友好，例如 \citet{zhu2016trained, cai2017deep}。
\end{enumerate}

针对上述挑战，本章提出了一种适用于目标检测任务且对于一般硬件友好的低比特量化神经网络训练及部署方法，称为\emph{用于目标检测的全量化网络}（\emph{Fully Quantized Network for Object Detection}，下文简称 FQN）。FQN 可以在复杂的目标检测模型——例如 RetinaNet~\citep{lin2017focal} 和 Faster RCNN~\citep{ren2015faster} 上——将模型参数及激活的数值精度量化至 4 比特，并保持足够的准确度。FQN 使用对于一般硬件友好的非对称线性量化，且通过 BN 折叠技术，使模型训练后不包含任何浮点操作。为解决 FQN 的训练收敛问题，本章详细分析了其在 MS COCO 数据集~\citep{lin2014microsoft} 上的训练过程，指出其训练不稳定性的根源在于模型参数的通道间分布差异、模型激活量化的准确度不足以及小训练批量尺寸导致的 BN 统计量不稳定，并提出了相应的措施解决上述问题。

本章在 MS COCO 数据集上验证了 FQN 的有效性，所用的检测算法包括一阶检测算法 RetinaNet 和二阶检测算法 Faster RCNN，所用的模型包括使用特征金字塔（Feature Pyramid Network~\citep{lin2017feature}，下文简称 FPN）的 ResNet-\{18, 34, 50\}~\citep{He_2016} 和 MobileNet-v2~\citep{Sandler_2018}。FQN 在上述实验中均保持了较高准确度，例如使用 4-bit ResNet-18 的 RetinaNet 检测模型应用 FQN 训练后，其 mAP 相较全精度模型仅下降 $0.031$ （4-bit $0.286$ mAP 相较于全精度 $0.317$ mAP），而之前被用于 TensorFlow Lite 中的 \citet{jacob2018quantization} 和 \citet{krishnamoorthi2018quantizing} mAP 下降分别为 $0.120$ 和 $0.091$，其相对准确度损失是 FQN 的 $3.87\times$ 和 $2.94\times$。
% ------------------------------------------------------------------------------
%    Motivation
% ------------------------------------------------------------------------------
\subsection{工作动机}
\paragraph{对于一般硬件友好的全量化网络}
量化网络的大部分部署场景（包括软件及硬件）并不支持特殊的量化模式，例如 NVIDIA TensorRT~\citep{vanholder2016efficient} 在包含 Tensor Core 的 GPU 或加速卡上运行 8-bit/4-bit 模型时，只支持对称线性量化~\citep{migacz20178}，Qualcomm SNPE~\citep{qualcomm2019snpe} 在 DSP 或 AIX 设备上运行 8-bit 模型时，只支持零点对齐的非对称线性量化。这意味着，目前许多使用特殊量化函数的模型量化研究~\citep{zhu2016trained, cai2017deep, Zhang_2018} 是对一般硬件不友好且难以实际部署的。同时，大部分现有研究过于关注量化模型的最终准确度，将量化模型中的敏感操作——例如模型第一层和最后一层、模型中的 BN 等正则化层等——保留为浮点数值精度~\citep{zhou2016dorefanet, Zhang_2018, li2019additive}。在实际的部署场景中，智能相机等终端设备内嵌的 FPGA 或 ASIC 等专有硬件为保证硬件利用率一般不会再额外设计浮点运算逻辑；移动设备端的神经网络推理库虽然支持将模型中部分操作从 DSP 或 AIX 等量化设备回调（fallback execution）至支持浮点运算的 CPU 或 GPU，但其中涉及的上下文切换会极大降低模型的运行效率。因此，探索针对一般硬件友好的、完全量化的模型量化方法有极大的潜在实际影响力。

\paragraph{适用于复杂任务的低比特量化网络}
目前神经网络模型量化研究主要集中在两个方向：
\begin{enumerate}[1)]
  \item 在相对简单的模型、任务和数据集上，使用极低的数值精度量化模型参数及激活。例如 \citet{hubara2016binarized, zhou2016dorefanet} 报告了在 SVHN、CIFAR-10 等物体分类任务上，可将 small-VGG、AlexNet 等模型参数及激活压缩至 1-bit；
  \item 使用较高的数值精度，量化用于相对复杂任务的模型。例如 \citet{jacob2018quantization} 报告了在人脸特征抽取、人脸检测、通用目标检测任务上，多种模型可在 8-bit 精度下保持可用准确度。
\end{enumerate}

由于定点或整数乘法逻辑的计算开销与量化位宽呈超线性，且占计算能耗大部分的访存开销正比于量化位宽~\citep{han2017efficient}，8-bit 量化的资源开销对于计算资源非常受限的设备而言仍然过于高昂。因此一个自然的想法是，能否将 4-bit $\sim$ 1-bit 低比特压缩技术用于复杂任务和复杂模型上。然而，直接将 \citet{rastegari2016xnor, zhou2016dorefanet} 等量化训练方法用于复杂模型时，会出现量化训练无法收敛、最终模型准确度低的问题。本章通过观察复杂任务上量化模型的训练过程，发现其中存在 BN 层 EMA 统计值波动明显、量化激活存在较多离群点等不稳定现象。后续实验进一步发现消除这些不稳定性，可显著改善复杂任务上低比特量化模型的训练收敛性并提高模型最终准确度（详见第~\ref{sec::fqn::experiments} 节）。
% ------------------------------------------------------------------------------
%    Contributions
% ------------------------------------------------------------------------------
\subsection{主要贡献}
\begin{enumerate}
  \item 本章提出了 FQN，一种适用于目标检测模型的低比特量化方法。FQN 使用对一般硬件友好的线性量化方式，使量化后的目标检测模型可以使用整数计算逻辑进行推理，且量化后的模型不包含浮点操作，便于部署至计算资源受限的终端设备；
  \item 本章分析了 FQN 的训练过程，指出其在低比特量化训练中不稳定性的主要来源，并提出了相应解决方案；
  \item 本章在 MS COCO 数据集上，使用不同神经网络模型和目标检测算法验证了 FQN 的准确性。FQN 在实验中达到了高于先前领域最优方法的检测准确度。
\end{enumerate}
% ==============================================================================
%  Method
% ==============================================================================
\section{目标检测模型的量化} \label{sec::fqn::methods}
本节详细描述 FQN 方法的各组成部分，包括对目标检测模型参数和激活的量化算法、量化模型的训练策略以及针对检测模型量化的实现细节。FQN 通过量化感知训练，可使检测模型在以低至 4-bit 的数值精度运行时仍保持足够的准确度。低比特量化目标检测模型的难点在于量化感知训练的不稳定性，而 FQN 通过对模型参数、激活、正则化层的量化算法和训练策略的改进以减少该不稳定性。FQN 对模型参数和激活使用线性量化，量化后仅需整数数值逻辑即可完成模型推理，从而便于部署至一般硬件平台上。
% ------------------------------------------------------------------------------
%    Quantization aware training
% ------------------------------------------------------------------------------
\subsection{量化感知训练} \label{sec::fqn::qat_overview}
根据~\citet{krishnamoorthi2018quantizing} 提出的针对模型量化方法的分类标准，FQN 属于\emph{量化感知训练}（\emph{Quantization Aware (re-)Training}，下文简称 QAT），即在模型训练时将模型参数及激活按照部署时的量化方式映射至离散浮点值，以模拟模型量化运行时的数值误差，从而训练得到在量化误差存在时仍能在目标任务上保持足够准确度的模型。采用 QAT 的模型量化训练部署流程依次包含下三个阶段：
\begin{description}
  \item[模型预训练] 在没有预先提供全精度预训练参数时，需要对模型进行预训练。对于物体分类等相对简单的任务，模型在以特定方式初始化后直接在目标数据集上训练；对于目标检测、场景分割等相对复杂的任务，则一般先将其主干模型（backbone network）在分类任务上训练至收敛，再加入任务子模型在目标数据集上训练。模型参数和激活在预训练过程中保持全精度。对于包含 BN 等正则化操作的模型，模型每层在训练时会统计在当前的小批量输入下，该层输出各通道的均值、方差 $\{\Batch{\mu}, \Batch{\sigma}\}$，使用该组统计值将其输出正则化为近似标准高斯分布，并通过动量 $m$ 更新该层输出均值、方差的指数移动平均（Exponential Moving Average，下文简称 EMA）统计值 $\{\EMA{\mu}, \EMA{\sigma}\}$；
  \item[模型量化感知训练] 在模型预训练结束，或事先已获得全精度预训练模型后，则对该全精度模型进行量化感知训练。量化感知训练在模型计算图中加入额外算子：对于模型各层参数和激活，在前向传播时将其离散化，以模拟量化部署运行时的数值误差；在反向传播时，则通过 Straight Though Estimator （下文简称 STE，详见第~\ref{sec::fqn::q_weight} 节）或其他方法，根据算子离散输出接收的梯度计算出其全精度输入的梯度，以便继续梯度传播。注意在此阶段，每组模型参数均保留一份全精度副本，以确保参数能被数值范围较小的梯度准确更新。
  
  在此阶段，先前基于 QAT 的主要工作~\citet{jacob2018quantization, krishnamoorthi2018quantizing} 在量化后的模型激活上计算其小批量统计值 $\{\Batch{\mu}, \Batch{\sigma}\}$ 并继续更新各层 EMA 统计值 $\{\EMA{\mu}, \EMA{\sigma}\}$。我们发现这是导致目标检测模型 QAT 不稳定的原因之一，详见第~\ref{sec::fqn::q_bn} 节；
  \item[模型量化部署] 在上一阶段完成后，所得模型已经可以在量化误差下保持足够准确度。在模型部署前，需将模型中 BN 层的参数——其 EMA 统计值 $\{\EMA{\mu}, \EMA{\sigma}\}$ 和仿射参数 $\{\alpha, \beta\}$ ——合并至前一层卷积或全连接层的参数 $\{W, b\}$ 中，消除额外的线性操作以提高模型推理效率。之后将模型中所有参数转换至部署平台所支持的量化格式，例如包含以 \verb|int32| 定点数表示量化缩放的 \verb|uint8| 矩阵，最终在目标平台上运行。注意量化后的模型也可以在 GPU 上以离散浮点形式使用 \verb|cuDNN| 等浮点计算库运行，以快速验证 QAT 后量化模型的准确度。
\end{description}
% ------------------------------------------------------------------------------
%    Asymmetric linear quantization
% ------------------------------------------------------------------------------
\subsection{非对称线性量化} \label{sec::fqn::quant_scheme}
目前大多数神经网络模型使用单精度浮点格式存储其模型参数，并在运行时使用浮点数值逻辑计算中间激活和最终输出。模型量化方法将这些浮点值舍入至事先定义的一组离散数值中，以减少模型的存储开销；若这些离散数值以线性均匀或 2 的幂次均匀（power-of-two uniform）排布，则运行时的计算可在离散数值的\emph{秩}上以整数数值逻辑进行。具体地，给定一浮点格式的张量 $X^R = [x^R_{1, \ldots n}]$ 和量化位宽 $k$，量化函数 $Q_k(\cdot)$ 将每一浮点值 $x^R_i$ 映射至距其最近的量化点 $q_j$：
\begin{align}
  X^Q = Q_k(X^R) \in \{q_1, \ldots q_{2^k-1}\} 
\end{align}

FQN 使用\emph{非对称线性量化}模式，即任意相邻 $q_j, q_{j+1}$ 间等距，但 $\{q_0, \ldots q_{2^k-1}\}$ 不关于零点对称。因此在 FQN 中，$X^Q$ 按照
\begin{align}
  X^Q = \Delta (X^I - z) \label{eq::fqn::asymm_linear_q}
\end{align}
计算得出。其中 $\Delta$ 表示任意 $q_j, q_{j+1}$ 间的距离（下文统称\emph{量化间距}），$X^I$ 是以无符号整数表示的、 $X^R$ 中各元素在量化点集中的秩，$z$ 表示实数零点对应至量化点集的秩。在给定量化数值范围 $[lb, ub]$ 和量化位宽 $k$ 后，$\Delta$ 按照
\begin{align}
  ub &= \max(ub, lb + \epsilon) \\
  \Delta &= \frac{ub - lb}{2^k - 1}
\end{align}
计算得出。其中 $\epsilon$ 为确保数值稳定性而设置的最小量化范围，一般设为 $10^{-2}$，且给定量化范围后 $q_1 = lb$，$q_{2^k-1} = ub$。在得到 $\Delta$ 后，$X^I$ 可按照
\begin{align}
  \tilde{X}^R &= \Clamp{X^R, lb, ub} \\
  X^I &= \Round{\frac{\tilde{X}^R - lb}{\Delta}}
\end{align}
计算得出。其中 $\Clamp{\cdot, a, b} = \max(a, \min(\cdot, b))$，将输入 $X^R$ 限制在量化范围内；$\Round{\cdot}: \mathbb{R}\to\mathbb{Z}$ 为舍入操作符。

当模型第 $l$ 层的参数 $W_l$ 和输入 $x_l$ 均被量化后，其数值操作变为
\begin{align}
  y_l = Q_k(W_l) Q_k(x_l) = \Delta_{W_l}\Delta_{x_l} (W^I_l x^I_l) \label{eq::fqn::intmm}
\end{align}
由于 $W^I_l, x^I_l$ 均以无符号整数表示，该层的数值运算可用更为高效的整数数值逻辑进行。注意~\eqref{eq::fqn::intmm} 中省略了对零点 $z$ 的处理，以及使用非浮点数值逻辑处理 $\Delta_{W_l}\Delta_{x_l}$ 的讨论。关于零点和缩放的算法及部署细节的详细讨论见第~\ref{sec::fqn::q_misc} 节。% 和第~\ref{sec::fqn::deployment} 节。

因为包含舍入操作 $\Round{\cdot}$ 的量化函数 $Q_k(\cdot)$ 并不可导，因此 FQN 在训练时使用 STE~\citep{bengio2013estimating} 回传上游梯度 $\diff{y}{X^I}$。注意超出量化范围 $[lb, ub]$ 的输入元素不接收梯度：
\begin{align}
  \diff{y}{x^R_i} = 
    \begin{cases}
      \diff{y}{x^I_i} & \text{if } x^R_i \in [lb, ub] \\
      0 & \text{otherwise}
    \end{cases}
\end{align}
% ------------------------------------------------------------------------------
%    Quantize weights
% ------------------------------------------------------------------------------
\subsection{模型参数量化} \label{sec::fqn::q_weight}

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{FQN/res50_channels.pdf}
    \caption{ResNet-50 模型参数各通道间最大/最小幅度比值的柱状统计，注意 x 轴单位为分贝}
    \label{img::fqn::w_channels}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{FQN/per_channel_quant.pdf}
    \caption{模型参数逐通道量化策略，每层各卷积核 $w_c$ 有各自的量化范围 $[lb_c, ub_c]$}
    \label{img::fqn::channel_quant}
  \end{subfigure}
  \caption{FQN 关于目标检测模型参数分布的分析~\subref{img::fqn::w_channels} 及对应的逐通道量化策略~\subref{img::fqn::channel_quant}。图~\subref{img::fqn::w_channels} 中分布在 60dB $\sim$ 80dB 之间的统计值说明模型中存在参数通道间幅度最大/最小比值达到 $10^6 \sim 10^8$ 的层，对这些层各通道使用同一量化边界将导致显著量化误差。因此，FQN 使用图~\subref{img::fqn::channel_quant} 所示的逐通道量化策略。}
  \label{img::fqn::w_quant}
\end{figure}

基于深度卷积网络的目标检测模型中，主要的参数化算子为卷积操作和全连接操作。对于 2D 卷积层，其参数 $W \in \mathbb{R}^{c_{\mathrm{out}} \times c_{\mathrm{in}} \times k_{\mathrm{H}} \times k_{\mathrm{W}}}$，对于全连接层，其参数 $W \in \mathbb{R}^{c_{\mathrm{out}} \times c_{\mathrm{in}}}$。其中 $c_{\mathrm{out}}$ 表示输出通道数（或称卷积核数），$c_{\mathrm{in}}$ 表示输入通道数（或称卷积核通道数），$k_{\mathrm{H}} \times k_{\mathrm{W}}$ 表示 2D 卷积核高宽尺寸。我们发现在目标检测模型中，同一层模型参数在 $c_{\mathrm{out}}$ 维度，或者说同一层不同卷积核之间，幅度范围相差非常明显。例如，在 MS COCO 数据集上训练的 ResNet-50 RetinaNet 检测模型，其 \verb|layer2.0.conv1| 层包含的卷积核中，参数幅度范围从 $3.745 \times 10^{−8}$ 至 $0.727$ 不等。若统计该模型各卷积层中，参数幅度最大的卷积核与参数幅度最小的卷积核之比，即对于模型第 $l$ 层，计算
\begin{align}
  \eta_l = \frac{\max(\mathrm{range} (W_l))}{\min(\mathrm{range}(W_l))} \label{eq::fqn::w_range_ratio}
\end{align}
其中 $\mathrm{range}(W_l)$ 返回第 $l$ 层中各卷积核参数 $W_{l, i}$ 的数值范围 $\max(W_{l, i}) - \min(W_{l, i})$。对 $\eta_{1, \ldots L}$ 做直方图统计，如图~\ref{img::fqn::w_channels} 所示，模型中存在大量参数通道间幅度差距悬殊的层。

如果模型每层参数共用一组量化范围，则数值范围较小的卷积核会因同层内其他数值范围较大的卷积核扩张量化范围，而产生巨大的相对数值误差，从而使 QAT 不稳定。为解决此问题，FQN 中同一层内各个卷积核使用各自的量化范围，如图~\ref{img::fqn::channel_quant} 所示，该量化方式称为\emph{逐通道量化}。具体地，模型第 $l$ 层参数 $W_l$ 各通道量化范围由该通道的最小值、最大值决定，即
\begin{align}
  lb_l &= \min_{\mathrm{dim}=c_{\mathrm{out}}} W_l \\
  ub_l &= \max_{\mathrm{dim}=c_{\mathrm{out}}} W_l
\end{align}
其中 $lb_l, ub_l \in \mathbb{R}^{c_{\mathrm{out}}}$。
% ------------------------------------------------------------------------------
%    Quantize activations
% ------------------------------------------------------------------------------
\subsection{模型激活量化} \label{sec::fqn::q_act}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\columnwidth]{FQN/act_quant.pdf}
  \caption{VGG16 主干模型第 12 层卷积输出的激活分布。注意该分布在 $[-1.5, -1.2]$ 和 $[1.2, 1.5]$ 范围内存在离群点，会导致激活量化范围过大而降低量化准确度，故 FQN 使用模型在标定集上激活分布的上下 $\gamma = 0.999$ 分位点作为该层激活的量化数值范围。}
  \label{img::fqn::q_act}
\end{figure}

为保证对一般硬件部署友好，FQN 量化了从正则化后的模型输入至检测任务子网络最后一层输入间的所有中间激活。由于事先无从得知模型运行时各中间激活的数值范围，故需在 QAT 阶段模型激活被量化前，对模型每一层中间激活进行\emph{标定}（\emph{calibration}）以确定其量化数值范围。\citet{jacob2018quantization, krishnamoorthi2018quantizing} 通过在模型激活被量化前的一部分训练步数中（\citet{jacob2018quantization} 在 ILSVRC 2012 分类任务中统计了 5K 个训练步数），使用 EMA 统计每层浮点激活在模型参数被量化后的数值范围，来作为该层激活被量化时的量化范围。我们在实验中发现，对于目标检测模型，使用 EMA 统计标定激活量化范围会有以下问题：
\begin{enumerate}[1)]
  \item 激活范围的 EMA 统计值对于\emph{统计步数}和\emph{EMA 动量}这两个超参数很敏感，针对不同的检测算法、主干模型和量化数值精度，需要做超参调优；
  \item 如图~\ref{img::fqn::q_act} 所示，目标检测模型的中间激活分布中容易包含\emph{离群点}，特别是在 FPN 输出特征图等不经过非线性层的中间激活。这些离群点容易将 EMA 动量扩张至较大范围，从而降低量化精度、使 QAT 更难收敛。
\end{enumerate}

针对上述问题，FQN 使用模型每层激活在标定集上的特定分布分位数作为该层激活的量化数值范围。具体地，FQN 在训练集上随机抽取 $n_{\mathrm{cal}}$ 组小批量样本作为\emph{标定集}，并将预训练得到的全精度模型以第~\ref{sec::fqn::q_weight} 节方法进行参数量化后，统计该模型在标定集上运行时各层激活分布的 $\gamma$、$1-\gamma$ 分位点，将其直接作为该层激活的量化范围，其中 $\gamma \in [0, 1]$。

我们在实验中发现，对于大部分目标检测模型 $n_{\mathrm{cal}} = 20, \gamma = 0.999$ 能使 QAT 训练得到最佳的效果。但更重要的是，第~\ref{sec::fqn::analysis} 节的实验说明，FQN 模型的最终准确度对超参数 $\gamma$ 的选取并不敏感。
% ------------------------------------------------------------------------------
%    Quantize BN
% ------------------------------------------------------------------------------
\subsection{BN 层折叠及量化} \label{sec::fqn::q_bn}
模型中的各 BN 层在训练阶段，通过统计该层在每个小批量输入下的输出激活 $x^l$ 的均值 $\Batch{\mu}$、方差 $\Batch{\sigma}$，将该输出正则化至近似标准高斯分布，同时通过动量因子更新 EMA 统计量 $\{\EMA{\mu}, \EMA{\sigma}\}$ 以在模型推理时使用。注意该正则化过程，以及后续的通过参数 $\{\alpha, \beta\}$ 进行的仿射变换，均是对该 BN 层输入的\emph{线性变换}。由于 BN 层与其之前的卷积或全连接层之间并无非线性操作，故 $\{\Batch{\mu}, \Batch{\sigma}, \alpha, \beta\}$ 可作为线性变换的参数，\emph{折叠}进先前层的参数 $\{W, b\}$ 中，从而在模型部署前去除显式的 BN 层。具体地，BN 前一层卷积或全连接层折叠后的参数 $\Fold{W}, \Fold{b}$ 为
\begin{align}
  \Fold{W} &= \frac{\alpha}{\sqrt{\Batch{\sigma}^2 + \epsilon}} W \label{eq::fqn::fold_w} \\
  \Fold{b} &= \frac{\alpha}{\sqrt{\Batch{\sigma}^2 + \epsilon}} (b - \Batch{\mu}) + \beta \label{eq::fqn::fold_b}
\end{align}
其中 $\epsilon$ 为保持计算数值稳定性的常数。在~\citet{jacob2018quantization, krishnamoorthi2018quantizing} 中，模型在训练阶段的训练计算图按照~\eqref{eq::fqn::fold_w} 及~\eqref{eq::fqn::fold_b} 修改，以模拟模型在部署运行时 BN 折叠并对 $\Fold{W}, \Fold{b}$ 量化后的数值误差。修改后，模型预训练阶段和量化感知训练阶段的计算图分别如图~\ref{img::fqn::foldbn_train} 和图~\ref{img::fqn::foldbn_quant_train} 所示。

\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{FQN/bn_beta_mean_var.pdf}
  \caption{4-bit ResNet-18 RetinaNet BN 层 \texttt{layer2.0.bn1} 在训练过程中仿射参数 $\beta$ （左）、逐批量激活均值 $\Batch{\mu}$ （中）和方差 $\Batch{\sigma}$ （右）分布的演化情况，其中 z 轴为训练步数。由于目标检测模型单 GPU 训练批量尺寸较小（每一 GPU 2 个样本）且模型激活被激进地量化，逐批量统计值 $\Batch{\mu}, \Batch{\sigma}$ 分布并不稳定。因此，FQN 在 QAT 阶段全程使用 EMA 统计值 $\EMA{\mu}, \EMA{\sigma}$ 对各 BN 层输入进行正则化。}
  \label{img::fqn::bn_stat}
\end{figure}

然而在目标检测模型的训练过程中，我们发现在小批次输入上统计的激活均值方差 $\Batch{\mu}, \Batch{\sigma}$ 很不稳定。例如图~\ref{img::fqn::bn_stat} 展示了 ResNnet-18 RetinaNet 模型在 MS COCO 数据集上训练时，其 \verb|layer2.0.bn1| BN 层中参数和统计值随训练步数的分布变化。其中间子图和右侧子图分别表示 $\Batch{\mu}, \Batch{\sigma}$ 的分布，其分布在训练过程中剧烈变化。我们指出这是导致量化目标检测模型 QAT 不稳定的主要原因之一。

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.7\columnwidth]{FQN/fold_bn.pdf}
  \caption{FQN 在 QAT 阶段对 BN 层做稳定化折叠。EMA 统计参数 $\EMA{\mu}, \EMA{\sigma}$ 来自全精度预训练模型，在 QAT 阶段和仿射参数 $\alpha, \beta$ 一并折叠至前一卷积层或全连接层参数 $W, b$ 中。QAT 阶段不再通过动量更新 $\EMA{\mu}, \EMA{\sigma}$ ，而 $\alpha, \beta$ 仍然通过梯度更新。}
  \label{img::fqn::foldbn_stable}
\end{figure}

FQN 通过一个简单的改进来处理此问题。注意在目标检测模型的训练流程中（见第~\ref{sec::fqn::qat_overview} 节的讨论），不论是事先提供的全精度模型或是在训练集上重新以全精度训练的模型，其中每一 BN 层均包含收敛模型的激活分布统计值 $\{\EMA{\mu}, \EMA{\sigma}\}$。我们发现使用该组在全精度下统计的 EMA 统计值也适用于 QAT 阶段的量化模型激活。因此在 FQN 的 QAT 阶段，模型各 BN 层均使用先前全精度模型的 EMA 统计值对该层激活做归一化操作。即在修改模型计算图时，将 $\{\EMA{\mu}, \EMA{\sigma}\}$ 而非 $\{\Batch{\mu}, \Batch{\sigma}\}$ 折叠至 BN 前一层的参数中。具体地，在 FQN 中 \eqref{eq::fqn::fold_w} 和 \eqref{eq::fqn::fold_b} 被修改为
\begin{align}
  \Fold{W} &= \frac{\alpha}{\sqrt{\EMA{\sigma}^2 + \epsilon}} W \label{eq::fqn::fold_w_stable} \\
  \Fold{b} &= \frac{\alpha}{\sqrt{\EMA{\sigma}^2 + \epsilon}} (b - \EMA{\mu}) + \beta \label{eq::fqn::fold_b_stable}
\end{align}
注意量化模型 BN 层的仿射参数 $\{\alpha, \beta\}$ 在 QAT 阶段仍保持更新。我们称这一方法为\emph{稳定化的 BN 折叠}，修改后的模型计算图如图~\ref{img::fqn::foldbn_stable} 所示。

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \begin{tikzpicture}[node distance=2cm,scale=0.7,transform shape]
        \node (conv) [conv] {conv};
        \node (weights) [params, below right of=conv] {weights};
        \node (input) [nobox, below left of=weights] {input};
        \node (moments) [act, above of=conv] {moments};
        \node (ma) [ma, above of=moments] {EMA $\mu$, $\sigma$};
        \node (fold_weights) [act, above left of=ma] {$w \gamma / \sigma$};
        \node (gamma) [small_params, below left of=fold_weights] {$\gamma$};
        \node (fold_conv) [conv, above of=fold_weights] {conv fold};
        \node (fold_biases) [act, above right of=ma] {$\beta - \gamma \mu / \sigma$};
        \node (beta) [small_params, below right of=fold_biases] {$\beta$};
        \node (add) [add, above of=fold_conv] {+};
        \node (ReLU6) [act, above of=add] {ReLU6};
        \node (output) [nobox, above of=ReLU6] {output};
        \draw [arrow] (input) -- (conv);
        \draw [arrow] (weights) -- (conv);
        \draw [arrow] (conv) -- (moments);
        \draw [arrow] (moments) -- (ma);
        \draw [arrow] (ma) -- (fold_weights);
        \draw [arrow] (gamma) -- (fold_weights);
        \draw [arrow] (ma) -- (fold_biases);
        \draw [arrow] (beta) -- (fold_biases);
        \draw [arrow] (fold_weights) -- (fold_conv);
        \draw [arrow] (input) to [out=135,in=225,looseness=1.1] (fold_conv);
        \draw [arrow] (fold_conv) -- (add);
        \draw [arrow] (fold_biases) -- (add);
        \draw [arrow] (add) -- (ReLU6);
        \draw [arrow] (ReLU6) -- (output);
    \end{tikzpicture}
    \caption{BN 层折叠至卷积层的训练计算图}
    \label{img::fqn::foldbn_train}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \begin{tikzpicture}[node distance=2cm,scale=0.7,transform shape]
      \node (conv) [conv] {conv};
      \node (weights) [params, below right of=conv] {weights};
      \node (input) [nobox, below left of=weights] {input};
      \node (moments) [act, above of=conv] {moments};
      \node (ma) [ma, above of=moments] {EMA $\mu$, $\sigma$};
      \node (fold_weights) [act, above left of=ma] {$w \gamma / \sigma$};
      \node (gamma) [small_params, below left of=fold_weights] {$\gamma$};
      \node (wt_quant) [quant, above of=fold_weights] {wt quant};
      \node (fold_conv) [conv, above of=wt_quant] {conv fold};
      \node (fold_biases) [act, above right of=ma] {$\beta - \gamma \mu / \sigma$};
      \node (beta) [small_params, below right of=fold_biases] {$\beta$};
      \node (add) [add, above of=fold_conv] {+};
      \node (ReLU6) [act, above of=add] {ReLU6};
      \node (act_quant) [quant, above of=ReLU6] {act quant};
      \node (output) [nobox, above of=act_quant] {output};
      \draw [arrow] (input) -- (conv);
      \draw [arrow] (weights) -- (conv);
      \draw [arrow] (conv) -- (moments);
      \draw [arrow] (moments) -- (ma);
      \draw [arrow] (ma) -- (fold_weights);
      \draw [arrow] (gamma) -- (fold_weights);
      \draw [arrow] (ma) -- (fold_biases);
      \draw [arrow] (beta) -- (fold_biases);
      \draw [arrow] (fold_weights) -- (wt_quant);
      \draw [arrow] (wt_quant) -- (fold_conv);
      \draw [arrow] (input) to [out=135,in=225,looseness=1.1] (fold_conv);
      \draw [arrow] (fold_conv) -- (add);
      \draw [arrow] (fold_biases) to [out=90,in=315] (add);
      \draw [arrow] (add) -- (ReLU6);
      \draw [arrow] (ReLU6) -- (act_quant);
      \draw [arrow] (act_quant) -- (output);
    \end{tikzpicture}
    \caption{BN 层折叠至卷积层中，且包含参数、激活量化算子的训练计算图}
    \label{img::fqn::foldbn_quant_train}
  \end{subfigure}
  \caption{BN 折叠后的模型训练计算图。模型在部署时，其中 BN 层参数 $\alpha, \beta, \EMA{\mu}, \EMA{\sigma}$ 会被“吸收”进前一卷积或全连接层的参数 $W, b$ 中，从而消除部署模型的显式 BN 算子，提高推理效率。为了在模型预训练和 QAT 阶段模拟 BN 折叠的效果，FQN 对训练计算图进行了相应修改。图片来自~\citet{jacob2018quantization}。}
  \label{img::fqn::foldbn_graphs}
\end{figure}
% ------------------------------------------------------------------------------
%    Misc
% ------------------------------------------------------------------------------
\subsection{量化算法其他细节} \label{sec::fqn::q_misc}
\paragraph{零点对齐}
在模型 QAT 和量化部署时，需要将 $X^R$ 中的零点准确映射至 $X^Q$ 中，以避免量化模型在对中间特征补零等操作时引入额外误差。在 FQN 中，通过在 QAT 每次量化前微调量化范围 $lb, ub$ 来将实数零点对齐至量化表示~\eqref{eq::fqn::asymm_linear_q} 中的零点 $z$。具体地，微调后的量化边界 $\tilde{lb}, \tilde{ub}$ 及对应的零点在量化点集 $\{q_1, \ldots q_{2^k-1}\}$ 中的秩 $z$ 为
\begin{align}
  z &= \Round{\frac{|lb|}{\Delta}} \\
  \tilde{lb} &= \sign(lb) \cdot \Delta z \\
  \tilde{ub} &= \sign(ub) \cdot \Delta (2^k - 1 - z)
\end{align}

\paragraph{上采样操作和元素间操作}
在包含 FPN~\citep{lin2017feature} 的目标检测模型中，主干网络多级特征由深至浅，依次通过上采样扩展至前一层特征尺寸大小后叠加输出，以使图片特征同时包含深层特征语意信息和浅层特征精确的空间信息。为确保不引入浮点操作，FQN 中所有上采样操作均使用最近邻插值完成。对于逐元素相加操作 $\Delta_a a^I + \Delta_b b^I = \Delta_c c^I$，需要将输入向量的缩放因子 $\Delta_a, \Delta_b$ 统一缩放至 $\Delta_c$，再对缩放后的 $c^I = \frac{\Delta_a}{\Delta_c}a^{I} + \frac{\Delta_b}{\Delta_c}b^{I}$ 使用整数数值逻辑相加。在 QAT 过程中，这一操作可以通过对逐元素操作的输入输出分别添加激活量化函数模拟。
% ==============================================================================
%  Deployment considerations
% ==============================================================================
% \section{真实世界硬件部署的考量} \label{sec::fqn::deployment}
% TODO
% ==============================================================================
%  Experiments
% ==============================================================================
\section{主要实验结果} \label{sec::fqn::experiments}
为验证 FQN 在目标检测任务上的有效性，本章报告使用不同目标检测算法和主干网络模型的 FQN 在 MS COCO 数据集上的实验结果。由于其标注类别的丰富性和样本场景的复杂性，MS COCO 是现今目标检测模型的主要评测数据集。在本章的所有实验中，模型准确性以其在 MS COCO 数据集\emph{物体边界框检测}（\emph{object bounding box detection}）任务上的多类别平均准确度（mean average precession，下文简称 mAP）和多类别平均召回率（mean average recall，下文简称 mAR）。实验结果中 mAP 和 mAR 计算方式按照 MS COCO 标准，报告了不同交并比（intersection over union，下文简称 IoU）阈值下的平均准确度 $\mAP{\{0.5:0.95, 0.5, 0.75\}}$，不同候选检测输出数目下的平均召回率 $\mAR{\{1, 10, 100\}}$，分别对于大、中、小尺寸目标的平均准确度和平均召回率 $\mAP{\{L, M, S\}}, \mAR{\{L, M, S\}}$。

\paragraph{训练策略}
本章中所有实验使用相同的训练策略。所有模型的预训练及 QAT 均在 MS COCO 数据集 \verb|coco-2017-train| 数据集上进行。在预训练阶段，模型主干网络使用 ImageNet 上预训练模型的参数作为初始化。预训练在 16 块 NVIDIA GTX 1080 Ti 上以同步（all-reduce）数据并行方式进行，每块 GPU 上训练的批量尺寸为 2。学习率线性 warmup 至 0.04，之后在训练第 30K 和第 80K 步分别将学习率衰减 $0.1\times$，并在 90K 步时结束。

在得到预训练至收敛的全精度模型后，QAT 阶段在相同的数据集上进行。模型参数和激活的数值精度被量化至 4-bit，各层激活量化范围在由 20 个从训练集上随机采样的小批量输入标定，量化上下界分别标定为每层激活分布的 $0.999\%$ 和 $0.001\%$ 分位点。QAT 阶段其他设置与预训练阶段一致，除了学习率被固定为 $0.004$。QAT 阶段持续 40K 步。
% ------------------------------------------------------------------------------
%    Numerical results
% ------------------------------------------------------------------------------
\subsection{数值结果} \label{sec::fqn::main_exp}

\begin{table}[p]
  \centering
  \caption{使用不同主干网络的一阶检测模型 RetinaNet FQN 在 MS COCO 数据集上的实验结果。表格中以 -FP32 结尾的数据表示作为基准的全精度模型的实验结果，以 -INT4 结尾的数据表示模型参数和激活数值精度被量化至 4-bit 的实验结果。注意由于 GPU 显存限制，训练 MobileNet-v2 模型时，输入图片短边尺寸为 600 像素。}
  \label{tab::fqn::retina_coco}
  \begin{subtable}[t]{\columnwidth}
    \centering
    \caption{RetinaNet 模型在 MS COCO 数据集上的平均准确率}
    \label{tab::fqn::retina_coco_mAP}
    \begin{tabular}{lc*{6}{c}}
      \toprule
      \multirow{2}{*}{模型} & \multirow{2}{*}{输入尺寸} & \multicolumn{6}{c}{mAP}  \\
      & & $\mathrm{AP}$ & $\mathrm{AP}^{0.5}$ & $\mathrm{AP}^{0.75}$ &
      $\mathrm{AP} ^ {\mathrm{S}}$ & $\mathrm{AP} ^ {\mathrm{M}}$ & $\mathrm{AP} ^ {\mathrm{L}}$ \\
      \midrule
      R50-FP32 & 800 & 0.356 &0.551 &0.382 &0.203 &0.393 &0.465 \\
      R50-INT4 & 800 & 0.325 &0.515 &0.347 &0.173 &0.356 &0.426 \\
      \hdashline
      R34-FP32 & 800 & 0.348 &0.538 &0.371 &0.192 &0.381 &0.460 \\
      R34-INT4 & 800 & 0.313 &0.504 &0.333 &0.161 &0.344 &0.416 \\
      \hdashline
      R18-FP32 & 800 & 0.317 &0.503 &0.337 &0.164 &0.346 &0.424 \\
      R18-INT4 & 800 & 0.286 &0.469 &0.299 &0.149 &0.312 &0.387 \\
      \hdashline
      MN2-FP32 & 600 & 0.275 &0.448 &0.290 &0.154 &0.289 &0.365 \\
      MN2-INT4 & 600 & 0.255 &0.425 &0.269 &0.134 &0.271 &0.345 \\
      \bottomrule
    \end{tabular}
  \end{subtable}
  \newline
  \vspace*{0.5 cm}
  \newline
  \begin{subtable}[t]{\columnwidth}
    \centering
    \caption{RetinaNet 模型在 MS COCO 数据集上的平均召回率}
    \label{tab::fqn::retina_coco_mAR}
    \begin{tabular}{lc*{6}{c}}
      \toprule
      \multirow{2}{*}{模型} & \multirow{2}{*}{输入尺寸} & \multicolumn{6}{c}{mAR} \\
      % \cline{3-14}
      & & $\mathrm{AR}^{1}$ & $\mathrm{AR}^{10}$ & $\mathrm{AR}^{100}$ &
      $\mathrm{AR} ^ {\mathrm{S}}$ & $\mathrm{AR} ^ {\mathrm{M}}$ & $\mathrm{AR} ^ {\mathrm{L}}$ \\
      \midrule
      R50-FP32 & 800 &0.308 &0.498 &0.529 &0.335 &0.569 &0.680 \\
      R50-INT4 & 800 &0.286 &0.463 &0.493 &0.298 &0.530 &0.643 \\
      \hdashline
      R34-FP32 & 800 &0.306 &0.493 &0.523 &0.319 &0.564 &0.686 \\
      R34-INT4 & 800 &0.284 &0.457 &0.487 &0.284 &0.524 &0.647 \\
      \hdashline
      R18-FP32 & 800 &0.288 &0.464 &0.495 &0.297 &0.529 &0.652 \\
      R18-INT4 & 800 &0.268 &0.433 &0.461 &0.269 &0.491 &0.621 \\
      \hdashline
      MN2-FP32 & 600 &0.267 &0.441 &0.470 &0.283 &0.492 &0.615 \\
      MN2-INT4 & 600 &0.253 &0.417 &0.445 &0.256 &0.468 &0.596 \\
      \bottomrule
    \end{tabular}
  \end{subtable}
\end{table}

\begin{table}[p]
  \centering
  \caption{使用不同主干网络的二阶目标检测模型 Faster RCNN FQN 在 MS COCO 数据集上的实验结果。与表~\ref{tab::fqn::retina_coco} 一致，表格中以 -FP32 结尾的数据表示作为基准的全精度模型的实验结果，以 -INT4 结尾的数据表示模型参数和激活数值精度被量化至 4-bit 的实验结果。注意由于 GPU 显存限制，训练 MobileNet-v2 模型时，输入图片短边尺寸为 600 像素。}
  \label{tab::fqn::faster_rcnn_coco}
  \begin{subtable}[t]{\columnwidth}
    \centering
    \caption{Faster RCNN 模型在 MS COCO 数据集上的平均准确率}
    \label{tab::fqn::faster_rcnn_coco_mAP}
    \begin{tabular}{lc*{6}{c}}
      \toprule
      \multirow{2}{*}{模型} & \multirow{2}{*}{输入尺寸} & \multicolumn{6}{c}{mAP}  \\
      & & $\mathrm{AP}$ & $\mathrm{AP}^{0.5}$ & $\mathrm{AP}^{0.75}$ &
      $\mathrm{AP} ^ {\mathrm{S}}$ & $\mathrm{AP} ^ {\mathrm{M}}$ & $\mathrm{AP} ^ {\mathrm{L}}$ \\
      \midrule
      R50-FP32 & 800 & 0.377 &0.593 &0.409 &0.220 &0.415 &0.489 \\
      R50-INT4 & 800 & 0.331 &0.540 &0.355 &0.182 &0.362 &0.436 \\
      \hdashline
      R34-FP32 & 800 & 0.358 &0.576 &0.384 &0.211 &0.390 &0.461 \\
      R34-INT4 & 800 & 0.318 &0.529 &0.339 &0.176 &0.344 &0.422 \\
      \hdashline
      R18-FP32 & 800 & 0.322 &0.538 &0.340 &0.180 &0.347 &0.419 \\
      R18-INT4 & 800 & 0.281 &0.484 &0.293 &0.145 &0.304 &0.381 \\
      \hdashline
      MN2-FP32 & 600 & 0.290 &0.497 &0.295 &0.160 &0.307 &0.390 \\
      MN2-INT4 & 600 & 0.255 &0.453 &0.257 &0.127 &0.275 &0.352 \\
      \bottomrule
    \end{tabular}
  \end{subtable}
  \newline
  \vspace*{0.5 cm}
  \newline
  \begin{subtable}[t]{\columnwidth}
    \centering
    \caption{Faster RCNN 模型在 MS COCO 数据集上的平均召回率}
    \label{tab::fqn::faster_rcnn_coco_mAR}
    \begin{tabular}{lc*{6}{c}}
      \toprule
      \multirow{2}{*}{模型} & \multirow{2}{*}{输入尺寸} &
      \multicolumn{6}{c}{mAR} \\
      & & $\mathrm{AR}^{1}$ & $\mathrm{AR}^{10}$ & $\mathrm{AR}^{100}$ &
      $\mathrm{AR} ^ {\mathrm{S}}$ & $\mathrm{AR} ^ {\mathrm{M}}$ & $\mathrm{AR} ^ {\mathrm{L}}$ \\
      \midrule
      R50-FP32 & 800 &0.316 &0.513 &0.541 &0.364 &0.580 &0.678 \\
      R50-INT4 & 800 &0.291 &0.468 &0.494 &0.320 &0.529 &0.629 \\
      \hdashline
      R34-FP32 & 800 &0.307 &0.496 &0.526 &0.348 &0.564 &0.611 \\
      R34-INT4 & 800 &0.284 &0.460 &0.486 &0.306 &0.520 &0.634 \\
      \hdashline
      R18-FP32 & 800 &0.286 &0.466 &0.494 &0.326 &0.524 &0.630 \\
      R18-INT4 & 800 &0.263 &0.429 &0.454 &0.281 &0.480 &0.594 \\
      \hdashline
      MN2-FP32 & 600 &0.272 &0.439 &0.465 &0.280 &0.502 &0.610 \\
      MN2-INT4 & 600 &0.250 &0.402 &0.426 &0.236 &0.465 &0.573 \\
      \bottomrule
    \end{tabular}
  \end{subtable}
\end{table}

表~\ref{tab::fqn::retina_coco} 和表~\ref{tab::fqn::faster_rcnn_coco} 分别报告了使用不同主干网络的一阶目标检测模型 RetinaNet FQN 和二阶目标检测模型 Faster RCNN FQN 在 MS COCO 验证集上的准确度。在这些表格中，以 R\{18, 34, 50\} 起始的数据行分别表示模型主干网络为包含 FPN 的 ResNet-\{18, 34, 50\}，以 MN2 起始的数据行表示模型主干网络为包含 FPN 的 MobileNet-v2。以 -FP32 结尾的数据行行表示作为基准的全精度模型，以 -INT4 结尾的数据行表示模型在 QAT 和量化部署阶段其参数和激活数值精度均被量化至 4-bit。

通过对比可以发现，4-bit 量化的 FQN 在一阶检测模型上表现最好。对于 MobileNet-v2 模型，其 4-bit 下的 mAP 仅比全精度基线下降 $0.020$，对于在目标检测任务中常用的 ResNet-50 和 -18 模型，其 4-bits 下 mAP 相较全精度基线分别仅下降 $0.031$ 和 $0.031$。考虑到 MobileNet 和 ResNet 系列模型本身已足够紧凑，且 4-bit 量化带来的存储和运算开销削减非常明显，这样的准确度下降与部署加速的权衡在实际模型应用场景中是可以接受的。

同时值得注意的是，在相同模型及量化精度下，二阶检测算法 Faster RCNN 的准确度下降均较一阶检测算法 RetinaNet 严重。如表~\ref{tab::fqn::faster_rcnn_coco_mAP} 报告的数据，使用 MobileNet-v2 主干网络的 Faster RCNN 在量化至 4-bit 后 mAP 下降 $0.035$，使用 ResNet-50 和 -18 的 Faster RCNN 则 mAP 分别下降 $0.046$ 和 $0.041$。我们认为这可能是由于 Faster RCNN 的检测任务子网络最后一层使用全连接层导致的。全连接层从计算特征上等效于不共享参数的 $1\times 1$ 2D 卷积，而~\citet{krishnamoorthi2018quantizing} 指出 $1\times 1$ 卷积对于量化敏感。
% ------------------------------------------------------------------------------
%    Comparison
% ------------------------------------------------------------------------------
\subsection{对比其他方法} \label{sec::fqn::comparison_exp}

\begin{table}[htb]
  \centering
  \caption{FQN 与其他模型量化方法在 MS COCO 数据集上的对比。所用模型为使用 ResNet-18 主干网络，且包含 FPN 的 RetinaNet 目标检测模型。除第 1 行为 FP32 全精度基线外，其他实验数据为模型参数及激活量化至 4-bit 后的结果。}
  \label{tab::fqn::baselines}
  \begin{tabular}{llc}
    \toprule
    量化方法 & 激活标定方法 & mAP \\
    \midrule
    \emph{全精度基线} & -- &0.317 \\
    \hdashline
    Integer-only~\citep{jacob2018quantization} & Moving average & 0.197 \\
    Quant whitepaper~\citep{krishnamoorthi2018quantizing} & Moving average & 0.226 \\
    DoReFa-Net~\citep{zhou2016dorefanet} foldBN & Clip to $[\{-1,0\}, +1]$ & 0.039 \\
    \hdashline
    \multirow{2}{*}{XNOR-Net~\citep{rastegari2016xnor} foldBN} & Moving average & 0.244 \\
    & Percentile & 0.267 \\
    \hdashline
    \textbf{Ours} & Percentile & \textbf{0.286} \\
    \bottomrule
  \end{tabular}
\end{table}

为进一步验证 FQN 的有效性，本节列出 FQN 与领域其他方法在相同设置下 MS COCO 数据集上的对比实验结果（见表~\ref{tab::fqn::baselines}），以及训练时的收敛速率（见图~\ref{fig::fqn::convergence}）。参与对比的其他方法包括 XNOR-Net~\citep{rastegari2016xnor}、DoReFa-Net~\citep{zhou2016dorefanet}、Google Integer-only~\citep{jacob2018quantization} 及 Quantization whitepaper~\citep{krishnamoorthi2018quantizing}。为在目标检测任务上公平对比，本节复现上述方法时做了下述修改：
\begin{enumerate}[1)]
  \item \citet{zhou2016dorefanet} 和 \citet{rastegari2016xnor} 原文提出的方法并未对 BN 做处理，而参与对比的其他方法均做了 BN 折叠及量化。因此本节实验中，使用 \citet{zhou2016dorefanet} 和 \citet{rastegari2016xnor} 方法量化的模型 BN 层也以相同方法折叠量化。其结果分别见表~\ref{tab::fqn::baselines} 第 $4 \sim 6$ 行；
  \item \citet{rastegari2016xnor} 原文提及了对于模型参数高于 1-bit 的量化方法，但是没有提及对于模型激活的标定方法。因此本节列出了使用指数移动平均和使用基于分位点的标定方法下，XNOR-Net 模型在 MS COCO 数据集上的 mAP，分别见表~\ref{tab::fqn::baselines} 第 5 行和第 6 行。
\end{enumerate}

通过对比可以发现，FQN 在目标检测这一任务下超过了领域内之前的所有最优方法。注意在~\citet{zhou2016dorefanet} 提出的方法中，模型所有激活被按照是否经过非线性层直接截断至 $[-1, 1]$ 或 $[0, 1]$。我们发现这样的激活截断策略在物体分类任务上几乎不损害模型准确度，但会使模型在目标检测任务上完全无法训练收敛，其最终 mAP 仅为 $0.039$。在与其他方法的对比中，我们发现在目标检测任务上，参数的逐通道量化对最终准确度关系明显：使用了逐通道参数量化的 \citet{krishnamoorthi2018quantizing, rastegari2016xnor} 方法，mAP 明显高于逐层参数量化的 \citet{jacob2018quantization}。

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.7\columnwidth]{FQN/mAP.eps}
  \caption{FQN 与其他模型量化方法 QAT 阶段收敛速度的比较。图为使用 ResNet-18 主干网络，且包含 FPN 的 RetinaNet 检测模型以 4-bit 数值精度在 MS COCO 数据集上的测试准确度曲线。x-轴为 QAT 步数，y-轴为模型在验证集上的 mAP。注意使用指数移动平均进行激活标定的方法在前 10K 步仅量化模型参数，故在其激活刚被量化时模型准确度会出现下跌。}
  \label{fig::fqn::convergence}
\end{figure}

本节还比较了 FQN 与上述基准方法在 QAT 阶段的收敛速度，如图~\ref{fig::fqn::convergence} 所示。对比实验在量化至 4-bit，使用 ResNet-18 主干网络且包含 FPN 的 RetinaNet 模型上进行。在 QAT 阶段，FQN 的收敛速度明显快于其他方法。注意使用指数移动平均进行激活标定的方法~\citep{jacob2018quantization, krishnamoorthi2018quantizing, rastegari2016xnor} 在 QAT 前 10K 步因统计激活范围仅做参数量化，故在第 10K 步——模型参数及激活同时量化——时，其在验证集上的准确度出现了下跌。
% ==============================================================================
%  Ablation study
% ==============================================================================
\section{模型简化实验及分析} \label{sec::fqn::analysis}
为验证 FQN 中各个部分的有效性，本节在 MS COCO 数据集上进行模型简化分析（ablation study）。所有实验均在使用ResNet-18 主干网络且包含 FPN 的 RetinaNet 模型上进行。我们分别验证了 FQN 相较于~\citet{jacob2018quantization, krishnamoorthi2018quantizing} 的改进在 QAT 阶段的叠加效果，以及 FQN 中的主要超参数——激活标定时分位点 $\gamma$ 的选取——的稳定性。 

简化实验的主要结果在表~\ref{tab::fqn::ablations} 中报告。由于 FQN 主要在~\citet{jacob2018quantization, krishnamoorthi2018quantizing} 上针对目标检测任务改进，而后两者主要关注通用 8-bit 模型量化，故本节分别报告了 4-bit （表~\ref{tab::fqn::ablation_4bits}）和 8-bit （表~\ref{tab::fqn::ablation_8bits}）下的模型简化结果。在这两组实验中，表格第 1 行均代表全精度模型的 mAP，之后每行代表包含某个或数个第~\ref{sec::fqn::methods} 节中提出的改进后的 mAP。具体地，F 表示第~\ref{sec::fqn::q_bn} 节讨论的稳定化 BN 折叠量化，P 表示第~\ref{sec::fqn::q_act} 节讨论的对模型激活使用基于分布分位点的方式标定，C 表示第~\ref{sec::fqn::q_weight} 节讨论的对模型参数做逐通道量化。表格中打 \checkmark 表示该行结果对应实验包含了该改进。表格第 2 行（不包含任何改进）代表~\citet{jacob2018quantization} 的实验结果，表格第 9 行代表本章提出的 FQN 的实验结果。

\begin{table}[htb]
  \centering
  \caption{在 MS COCO 上模型简化分析实验，结果为使用 ResNet-18 主干网络，且包含 FPN 的 RetinaNet 目标检测模型在 4-bit 和 8-bit 数值精度下的 mAP。在表~\subref{tab::fqn::ablation_4bits} 及表~\subref{tab::fqn::ablation_8bits} 中，格第 1 行均代表全精度模型的 mAP，之后每行代表包含某个或数个第~\ref{sec::fqn::methods} 节中提出的改进后的 mAP。具体地，F 表示第~\ref{sec::fqn::q_bn} 节讨论的稳定化 BN 折叠量化，P 表示第~\ref{sec::fqn::q_act} 节讨论的对模型激活使用基于分布分位点的方式标定，C 表示第~\ref{sec::fqn::q_weight} 节讨论的对模型参数做逐通道量化。表格中打 $\checkmark$ 表示该行结果对应实验包含了该改进。}
  \label{tab::fqn::ablations}
  \begin{subtable}{0.475\columnwidth}
    \centering
    \caption{4-bits ResNet-18 RetinaNet 的模型简化实验}
    \label{tab::fqn::ablation_4bits}
    \begin{tabular}{*{6}{c}}
      \toprule
      F & P & C & $\mathrm{AP}$ & $\mathrm{AP}^{0.5}$ & $\mathrm{AP}^{0.75}$ \\
      \midrule
      \multicolumn{3}{l}{\emph{全精度基线}} &0.317 &0.503 &0.337 \\
      \hdashline
      & & & 0.197 & 0.348 & 0.198 \\
      \checkmark& & & 0.235 & 0.402 & 0.245 \\
      & \checkmark& & 0.222 & 0.381 & 0.228 \\
      & & \checkmark& 0.250 & 0.419 & 0.260 \\
      \checkmark& \checkmark& & 0.254 & 0.426 & 0.265 \\
      \checkmark& & \checkmark& 0.268 & 0.442 & 0.280 \\
      & \checkmark& \checkmark& 0.273 & 0.449 & 0.288 \\
      \checkmark& \checkmark& \checkmark& \textbf{0.286} & \textbf{0.469} & \textbf{0.299} \\
      \bottomrule
    \end{tabular}
  \end{subtable}
  \quad
  \begin{subtable}{0.475\columnwidth}
    \centering
    \caption{8-bits ResNet-18 RetinaNet 的模型简化实验}
    \label{tab::fqn::ablation_8bits}
    \begin{tabular}{*{6}{c}}
      \toprule
      F & P & C & $\mathrm{AP}$ & $\mathrm{AP}^{0.5}$ & $\mathrm{AP}^{0.75}$ \\
      \midrule
      \multicolumn{3}{l}{\emph{全精度基线}} & 0.317 & 0.503 & 0.337 \\
      \hdashline
      & & &0.296 &0.475 &0.312 \\
      \checkmark & & &0.299 &0.481 &0.314 \\
      &\checkmark & &0.313 &0.499 &0.344 \\
      & &\checkmark &0.293 &0.473 &0.308 \\
      \checkmark &\checkmark & &0.312 &0.495 &0.311 \\
      \checkmark & &\checkmark &0.302 &0.482 &0.319 \\
      &\checkmark &\checkmark &0.312 &0.497 &0.311 \\
      \checkmark &\checkmark &\checkmark & \textbf{0.314} & \textbf{0.498} & \textbf{0.332} \\
      \bottomrule
    \end{tabular}
  \end{subtable}
\end{table}

\paragraph{稳定化的 BN 折叠}
对比表~\ref{tab::fqn::ablation_4bits} 和表~\ref{tab::fqn::ablation_8bits} 的第 2、3 行可以发现，在 QAT 阶段固定量化模型的 BN 层统计值可以提高模型训练后的准确度。具体地，4-bit 数值精度下，使用稳定化的 BN 折叠带来了 $+0.038$ 的 mAP 提升，8-bit 数值精度下带来 $+0.003$ 的少量提升。低比特量化下的提升较高比特量化明显，说明对模型激活的量化就会显著影响小批量激活分布统计的稳定性，即在量化数值精度较低时 $\{\Batch{\mu}, \Batch{\sigma}\}$ 会导致 QAT 不稳定而不应被 BN 层使用。

注意~\citet{krishnamoorthi2018quantizing} 也发现在模型 QAT 的最后阶段固定 BN 统计值可改善模型最终准确度。为验证 FQN 与~\citet{krishnamoorthi2018quantizing} 就 BN 固定策略在目标检测任务上的差异，本节就 BN 固定策略进行了对比实验。实验仍在 4-bit 和 8-bit ResNet-18 RetinaNet 上进行，分别按照 FQN 全程固定 BN 统计及按照 \citet{krishnamoorthi2018quantizing} 在 QAT 阶段最后 10K 步固定 BN。实验结果见表~\ref{tab::fqn::freeze_bn_compare} 所示。在不同量化数值精度下，QFN 所用的全程固定 BN 统计值的策略能得到 mAP 更高的模型，且在低比特量化时收益更为明显（$+0.010$ mAP ）。

\begin{table}[htb]
  \centering
  \caption{不同 BN 统计值固定策略在不同数值精度下的对比实验。勾选 F 指在整个 QAT 过程中固定所有 BN 层 EMA 统计值 $\EMA{\mu}, \EMA{\sigma}$，否则按照 \citet{krishnamoorthi2018quantizing} 仅在 QAT 最后 10K 步固定 EMA 统计值。}
  \label{tab::fqn::freeze_bn_compare}
  \begin{tabular}{l*{4}{c}}
    \toprule
    Precision & F & $\mathrm{AP}$ & $\mathrm{AP}^{0.5}$ & $\mathrm{AP}^{0.75}$ \\
    \midrule
    \multirow{2}{*}{4 bits} & & 0.226 & 0.384 & 0.235 \\
    & \checkmark & \textbf{0.236} & 0.400 & 0.245 \\
    \hdashline
    \multirow{2}{*}{8 bits} & &0.299 & 0.479 & 0.316 \\
    & \checkmark & \textbf{0.300} & 0.480 & 0.318 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{基于分布分位点的激活标定}
对比表~\ref{tab::fqn::ablation_4bits} 和表~\ref{tab::fqn::ablation_8bits} 的第 2、4 行可以发现，基于分位点的激活标定较基于移动平均的激活标定能在不同数值精度下显著提高量化模型 QAT 后的准确度。在 4-bit 模型上，使用基于分位点的激活标定带来了 $+0.225$ 的 mAP 提升；在 8-bit 模型上，此改进也能带来 $+0.017$ 的 mAP 提升。这说明不论量化数值精度高低，目标检测模型的激活中均包含可能降低量化准确度的离群点，需要在激活范围标定时予以剔除。

\begin{table}[htb]
  \centering
  \caption{激活标定过程中，统计分位点 $\gamma$ 的不同取值对模型最终准确度的影响。}
  \label{tab::fqn::gamma_compare}
  \begin{tabular}{*{4}{c}}
    \toprule
    $\gamma$ & $\mathrm{AP}$ & $\mathrm{AP}^{0.5}$ & $\mathrm{AP}^{0.75}$ \\
    \midrule
    0.9990 & 0.286 & 0.469 & 0.299 \\
    0.9973 & \textbf{0.289} & 0.469 & 0.308 \\
    0.9545 & 0.275 & 0.449 & 0.287 \\
    \bottomrule
  \end{tabular}
\end{table}

另一需要注意的点是 FQN 中对激活标定方法包含的超参数 $\gamma$，即统计激活分布时采用的分位点。第~\ref{sec::fqn::experiments} 节中的实验均选取了 $\gamma = 0.999$，本节对此超参数的分析实验进一步指出，FQN 模型的最终性能对 $\gamma$ 的选取并不敏感。为验证 $\gamma$ 取值对模型准确度的影响，本节的对比实验采用的不同的 $\gamma$ 取值：除上文所用的 $0.999$ 外，还验证了 $\gamma = 0.9973$ 和 $\gamma = 0.9545$（即假设模型激活随机且服从 $\NormalDist{\mu, \sigma}$ 分布，则在 $[\mu-3\sigma, \mu+3\sigma]$ 间的激活值占总体的 $0.9973$，在 $[\mu-2\sigma, \mu+2\sigma]$ 间的激活值占总体的 $0.9545$）。如表~\ref{tab::fqn::gamma_compare} 报告的结果，对 $\gamma$ 的不同取值并不会显著影响模型最终的 mAP。

\paragraph{模型参数逐通道量化}
对比表~\ref{tab::fqn::ablation_4bits} 的第 2、4 行可以发现，模型参数逐通道量化在低比特量化时对提高模型准确度有明显帮助。4-bit 量化时，对模型参数使用逐通道量化带来了 $+0.053$ 的 mAP 提升。对比该表的第 6、9 行可以发现，即使在使用了稳定化 BN 折叠和改进激活标定后，使用逐通道量化仍能带来 $+0.032$ 的 mAP 提升。
% ==============================================================================
%  Conclusion
% ==============================================================================
\section{结论}
本章提出了 FQN，一种仅使用整数数值逻辑即可完成部署的、端到端量化的目标检测模型量化方案。FQN 可以将目标检测模型的参数和激活量化至 4-bit 整数，且在部署运行时不包含任何浮点操作。相较于领域内其他方法，FQN 可在各种常用且紧凑的目标检测算法和主干模型上做到低比特压缩，且保持接近全精度模型的准确率。本章详细测试并报告了 FQN 在一系列检测模型上的性能，希望能以此促进学术界和工业界对于轻量化目标检测算法和部署技术的研究。
