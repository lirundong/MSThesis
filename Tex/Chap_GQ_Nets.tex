\chapter{量化网络的反向蒸馏训练} \label{chap::gq_nets}
% ==============================================================================
%  Introduction
% ==============================================================================
\section{引言}
第~\ref{chap::fqn} 章针对 FQN 模型的分析显示，基于量化神经网络的目标检测模型训练较一般模型困难，其原因在于量化误差为目标检测模型训练引入了更多不稳定性。若针对此类误差建模，并在训练过程中显式地优化该误差，则有可能得到更为鲁棒的量化目标检测模型。

本章介绍一种将神经网络量化误差以可微分形式隐式建模，并利用类似模型蒸馏训练的反向蒸馏技术，将该误差模型引入原网络训练过程的训练方法。该方法可平行整合至原模型训练方法中，将模型训练过程引导至同时最小化原损失函数和量化误差的求解点，从而得到适合被量化运行的神经网络模型。我们称使用该方法训练的量化神经网络为 GQ-Nets (Guided Quantization Networks)。

GQ-Nets 使用在相同输入下，量化神经网络和全精度浮点神经网络之间最终输出的不一致性作为量化误差的度量。在分类任务中，该不一致性由各神经网络最后一层输出（即 logits）间的 KL--散度刻画；在检测任务中，则由模型分类子网络输出的 logits 间的 KL--散度，以及回归子网络输出间的 smooth L1-loss \unsure{也许应该使用 L2-loss？} 一同刻画。这类一致性标准虽然没有针对模型参数及量化参数的显式表达式，但通过在量化操作的反向传播过程中引入 STE，其计算过程变得可微分，从而使得量化误差能够在训练过程中被显式地优化。

在将针对量化误差的优化引入原模型训练过程后，原模型优化问题演化为一阶多目标优化问题，即需要考虑最小化量化误差的梯度是否会与原模型训练梯度冲突。通过在模型训练过程中对这两组梯度间余弦相似度的度量，揭示了这两组梯度在训练的多数过程中近似正交；即对于大部分任务而言，减少模型量化误差这一任务并不与模型原始任务冲突。基于上述观察，本章引入了调和这两组梯度的一系列训练方法，并对其效果进行了分析比较。
% ------------------------------------------------------------------------------
%    Motivation
% ------------------------------------------------------------------------------
\subsection{工作动机}
\paragraph{对量化误差的端到端建模}
之前工作对神经网络量化过程的误差建模多集中在逐层或逐张量误差分析，以及依赖特定显式一致性指标，例如（……）。然而，在较为复杂的神经网络中，这可能导致两方面的问题：
\begin{enumerate}[1)]
  \item 逐层或逐张量误差分析可能不够准确：例如在 MobileNet 系列模型中，由于层参数量较少导致各卷积核表达能力受限，使用 depth-wise convolution 的中间层会产生较大的参数量化误差和激活量化误差，然而模型最终的误差却仍可能被控制在较小范围内（……）；
  \item 使用某一特定的显式一致性指标不一定能适应各种任务：例如在目标检测的回归子网络中，其输出的一致性指标一般用 L2-loss 或 smooth L1-loss 刻画；在嵌入向量的非监督学习中，则一般用余弦相似度刻画嵌入向量间的一致性；
\end{enumerate}
因此我们提出，应针对所用神经网络模型的任务使用相应的一致性指标，并以\emph{相同输入}下，全精度模型和量化模型\emph{最终输出}间的一致性，作为量化误差的衡量标准。也就是说，相较于模型训练或推理时某些中间层的局部误差，我们更关注模型最终输出间的不匹配度。

\paragraph{在模型预训练阶段即引入量化误差}
之前基于量化感知训练的工作，一般采取“预训练——量化感知微调”的两段式训练方法来减少神经网络的量化误差。然而，这类训练流程的代价是训练时间消耗几乎是传统训练的 $2\times$ 以上，且在量化感知微调阶段需要模型的完整训练数据集。例如在 LIQ 的 ImageNet 分类任务上，模型先在预训练阶段训练 120 轮，之后再在量化感知微调阶段训练 $60\sim 90$ 轮。如果考虑在量化感知微调阶段，模型计算图中包含的更多额外操作（例如量化、反量化、量化 STE 算子的梯度修改等），则在 ImageNet 分类任务上 LIQ 的总训练时长约是一般全精度模型的 $2\times$ 左右。与此同时，在实际应用场景中，模型在量化部署前一般很难接触到完整的训练数据集（例如模型的设计、训练和部署分别由不同的技术团队负责，各团队间数据不一定共享），这直接导致了两段式量化感知训练在实际场景中不一定可行。因此在 GQ-Nets 在能够接触完整训练数据的模型预训练阶段，即开始考虑模型量化运行的误差，并直接在模型参数优化时，尽可能消除该误差。

\paragraph{调和训练阶段多任务梯度}
对于较复杂的神经网络模型而言，其训练过程已包含来自不同任务损失函数的梯度——例如在用于实例分割的 Mask R-CNN 训练过程中，模型参数同时被最小化分类损失、检测锚框回归损失和分割掩模的三组梯度更新。在 GQ-Nets 的训练过程中，最小化量化误差会为模型参数生成另一组梯度。使用多组随训练过程频繁变化的梯度，优化同一组模型参数是较为困难的。因此，GQ-Nets 提出的在预训练阶段减小量化误差的挑战之一是，调和各损失函数的对应梯度，使模型能够收敛至同时最小化所有损失函数的解。
% ------------------------------------------------------------------------------
%    Contributions
% ------------------------------------------------------------------------------
\subsection{主要贡献}
\begin{enumerate}[1.]
  \item 本章提出了 GQ-Nets，一种使用量化神经网络与全精度神经网络间端到端不匹配度作为量化误差建模，并将该误差引入模型预训练阶段，从而直接得到量化友好的神经网络模型的训练方法；
  \item 本章深入分析了 GQ-Nets 在训练阶段的诸多特性，提出并验证了调和多任务训练不同梯度的多个方法；
  \item 本章在不同任务和大量真实数据集上验证了 GQ-Nets 的有效性，包括在 CIFAR、ImageNet 数据集上的图片分类任务，以及在 MS~COCO \improvement{在 mmdetectin 上补实验} 数据集上的多类别目标检测任务。
\end{enumerate}
% ==============================================================================
%  Modeling Quantization Errors
% ==============================================================================
\section{量化误差的建模} \label{sec::gq_nets::q_error}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{GQ_Nets/arch.pdf}
  \caption{GQ-Nets 的主要结构。其中，……}
  \label{img::gq_nets::arch}
\end{figure}

为了能在预训练阶段减小神经网络模型的量化误差，GQ-Nets 将神经网络模型的量化误差构造为：在初始全精度神经网络模型的基础上，使用同一套模型参数\emph{并行地}构建一个量化神经网络模型，将它们在同一输入下的最终输出与全精度模型的对应输出相类比，作为量化误差的衡量指标。GQ-Nets 的主要结构如图~\ref{img::gq_nets::arch} 所示，其中的前向传播（forward pass）部分展示了在预训练阶段对量化误差 $\mathcal{L}_f$ 及模型整体损失函数 $\mathcal{L}$ 的构造，反向传播（backward pass）部分展示了采用整体损失函数 $\mathcal{L}$ 对模型参数做量化感知训练的细节。本节先详细讨论前向传播过程中模型量化误差的构造方式，第~\ref{sec::gq_nets::train} 节讨论该框架的具体优化细节。

在前向传播过程中，GQ-Nets 各部分及其运行流程如下：
\begin{enumerate}[1.]
  \item 初始 $L$ 层全精度神经网络模型 $f_{\mathcal{W}}$，其中 $\mathcal{W} = \{W_1, \ldots, W_L\}$ 表示模型的权重参数（下文统称为\emph{权重参数}），而 $W_i, i \in 1 \ldots L$ 分别表示模型第 $i$ 层的权重参数，均以浮点形式存储及运算；
  \item 由 $f_{\mathcal{W}}$ 构建而来的 $L$ 层量化神经网络模型 $\hat{f}_{\mathcal{W}, \mathcal{Q}}$，其中 $\mathcal{Q}$ 表示量化模型在运行时所用的量化函数。 具体地，$\mathcal{Q} = \{Q^w_1, \ldots , Q^w_L, Q^a_1, \ldots , Q^a_L\}$ ，其中 $Q^w_i, i \in 1\ldots L$ 表示用于量化第 $i$ 层权重参数的量化函数，$Q^a_i, i \in 1\ldots L$ 表示用于量化第 $i$ 层模型激活的量化函数。注意在 GQ-Nets 中，每一个量化函数都分别被一组参数 $\theta^w_i$ （用于参数量化函数 $Q^w_i$ ）或 $\theta^a_i$ （用于激活量化函数 $Q^a_i$ ）参数化。每组参数 $\theta = \{k, lb, ub\}$ ，\footnote{此处省略 $\theta$ 上下标。}包括量化比特位宽 $k \in \mathbb{Z}^+$，量化截断上下界 $ub, lb \in \mathbb{R}$，其中 $ub, lb$ 是可学习参数。下文统称 $\Theta = \{\theta^w_1, \ldots , \theta^w_L, \theta^a_1, \ldots , \theta^a_L\}$ 为\emph{量化参数}。本章中所有参数化量化函数 $Q$ 均使用带截断的线性量化，即给定一输入向量 $x$ （可以是模型参数、输入或中间激活）和量化参数 $\theta = \{k, lb, ub\}$，其对应量化输出 $\hat{x}$ 为
  \begin{align}
    \Delta &= \frac{ub - lb}{2^k - 1} \\
    \hat{x} &= Q(x; k, lb, ub) \notag \\
            &= \Delta \lfloor \frac{\mathrm{clamp}(x, lb, ub) - lb}{\Delta} \rceil + lb \label{eq::gq_nets::linear_q_func}
  \end{align}
  其中 $\mathrm{clamp}(x, lb, ub) = \max(lb, \min(x, ub))$ 为截断操作符，在实际量化前将 $x$ 所有数值范围控制在 $[lb, ub]$ 范围内；$\lfloor\cdot\rceil: \mathbb{R}\to\mathbb{Z}$ 为舍入操作符。注意 $\hat{f}_{\mathcal{W}, \mathcal{Q}}$ 通过权重参数量化函数 $Q^w_{1\ldots L}$，与 $f_{\mathcal{W}}$ 共享一套权重参数 $\mathcal{W}$；
  \item GQ-Nets 模型各层在前向传播时，同时计算浮点和量化的中间激活值。具体地，在得到一经过预处理的、以浮点格式存储的小批量输入 $x_0$ 后，第 $i=1$ 层首先计算该层的浮点输出激活 $x_1 = x_0 \star_1 W_1$，其中 $\star_1$ 表示该层的对应操作（例如卷积、逐通道分离卷积或池化等）；之后将该层的输入 $x_0$、权重参数 $W_1$ 使用对应的量化函数及量化参数进行量化 $\hat{x}_0 = Q^a_1(x_0; \theta^a_1), \hat{W}_1 = Q^w_1(W_1; \theta^w_1)$，即可计算得到该层的量化输出激活 $\tilde{x}_1 = \hat{x}_0 \star_1 \hat{W}_1$。对于 $i\in 2\ldots L$ 层，浮点和量化前向计算不再共享输入，即分别使用 $i-1$ 层的浮点输出激活 $x_{i-1}$ 和量化输出激活 $\tilde{x}_{i-1}$ 计算本层的浮点输出 $x_i = x_{i-1} \star_i W_i$ 和量化输出 $\tilde{x}_i = Q^a_i(\tilde{x}_{i-1}; \theta^a_i) \star_i Q^w_i({W}_{i}; \theta^w_i)$ 。注意各层在执行量化前向计算时，由于其输入和权重参数均已被量化，故 $\tilde{x}_i$ 可完全由定点或整数数值逻辑计算得出；
  \item 在得到第 $L$ 层的浮点输出 $x_L$ 和量化输出 $\tilde{x}_L$ 后，分别计算反映模型在目标任务上准确度的原损失函数 $\mathcal{L}_f$ 及反映模型量化输出与浮点输出间相似程度的量化误差度量 $\mathcal{L}_q$，并由此计算模型训练的总体损失函数 $\mathcal{L}$。不失一般性地，本章中使用线性加权计算 $\mathcal{L}$，即
  \begin{align}
    \mathcal{L} = w_f \mathcal{L}_f + w_q \mathcal{L}_q \label{eq::gq_nets::total_loss}
  \end{align}
  其中 $w_f, w_q \in \mathbb{R}$ 分别表示模型训练过程对准确度和量化误差减少的偏重程度，并可以在训练过程中动态调整。对于分类任务，$\mathcal{L}_f$ 一般表示为模型最终浮点输出 $x_L$ 对应的概率分布与当前小批量输入标签 $y$ 间的交叉熵 $\mathcal{L}_f = \CE{\sigma(x_L)}{y}$，而 $\mathcal{L}_q$ 则表示为模型量化输出对应的概率分布相较于浮点输出对应的概率分布间的 KL--散度 $\mathcal{L}_q = \KL{\sigma(\tilde{x}_L)}{\sigma(x_L)}$，其中 $\sigma(\cdot)$ 表示 softmax 操作。对于回归任务（例如目标检测中检测框坐标尺寸），$\mathcal{L}_f$ 一般表示为回归子网络输出与检测框标定间的 smooth L1-loss ， $\mathcal{L}_q$ 一般表示为全精度和量化回归子网络输出间的 L2-loss 。
  
  通过构造损失函数~\eqref{eq::gq_nets::total_loss} 的第二项，GQ-Nets 在模型预训练阶段即可刻画出当前模型的量化误差。在此误差的构造过程中，我们只关注量化模型\emph{最终}输出与全精度输出的近似性，为模型保留了更多优化空间。\ref{sec::gq_nets::train} 节将详细讨论针对该误差模型的优化方法。
\end{enumerate}
% ==============================================================================
%  Method
% ==============================================================================
\section{反向蒸馏训练} \label{sec::gq_nets::train}
\improvement{在这里详细说明为什么叫“反向蒸馏”训练}
GQ-Nets 在预训练阶段构造模型量化误差~\eqref{eq::gq_nets::total_loss} 中 $\mathcal{L}_q$ 时，前向传播运算的几乎所有操作算子均可微分，并且模型的所有权重参数 $\mathcal{W}$ 及量化参数 $\Theta$ 均参与了构造。因此，以适当的优化方法最小化~\eqref{eq::gq_nets::total_loss} 即可以得到一套量化友好的模型权重 $\mathcal{W}$ 及与之对应的量化运行时参数 $\Theta$。使用基于梯度的方法优化~\eqref{eq::gq_nets::total_loss} 需要考虑以下问题：
\begin{enumerate}[1)]
  \item 在训练过程中权衡 $\mathcal{L}_f$ 与 $\mathcal{L}_q$ 间的重要性，防止两者梯度在训练时互相冲突，或训练由某一组梯度完全主导，以确保训练方法能得到在目标任务上足够准确、同时量化后额外误差最小的模型；
  \item 量化模型 $\hat{f}_{\mathcal{W, Q}}$ 计算图在反向传播时的额外细节，例如 $\lfloor\cdot\rceil$ 操作默认不可微分、针对 $\mathcal{L}_q$ 的优化可能降低 $f_{\mathcal{W}}$ 的准确度等；
  \item 浮点和量化模型在训练过程中，其激活分布可能会不一致，需要使用相应的方法分别做正则化处理。
\end{enumerate}
本节讨论针对上述问题的一系列解决方案，使 GQ-Nets 可以在不显著修改原模型 $f_{\mathcal{W}}$ 的基础上，训练得到高准确度且无需额外微调的量化友好模型 $f_{\mathcal{W, Q}}$。
% ------------------------------------------------------------------------------
%    Loss function
% ------------------------------------------------------------------------------
\subsection{损失函数设计} \label{sec::gq_nets::loss_func}
正如在第~\ref{sec::gq_nets::q_error} 节所讨论的，GQ-Nets 损失函数~\eqref{eq::gq_nets::total_loss} 的构造思路是，通过优化原模型损失函数 $\mathcal{L}_f$ 使模型在目标任务上保持高准确度，同时优化量化误差项 $\mathcal{L}_q$ 使模型在量化后不产生明显的输出误差。然而需要注意的是，直接优化 $\mathcal{L}_q$ 会使 $x_L$ 和 $\tilde{x}_L$ 趋向一致，这一过程可能会使浮点输出对应的概率分布 $\sigma(x_L)$ 偏离目标分布 $y$，从而与 $\mathcal{L}_f$ 的优化冲突。

为解决此问题，GQ-Nets 在构造 $\mathcal{L}_q$ 时，将浮点输出对应的概率分布 $\sigma(x_L)$ 看作\emph{常数项}，从而其梯度 $\nabla_{\mathcal{W}, \Theta} \mathcal{L}_q$ 只会通过量化模型的输出 $\tilde{x}_L$ 作用于 $\hat{f}_{\mathcal{W, Q}}$，使量化模型输出对应概率 $\sigma(\tilde{x}_L)$ 趋近于 $\sigma(x_L)$，而不会对 $\sigma(x_L)$ 的准确度产生负面影响。虽然 $\nabla_{\mathcal{W}, \Theta} \mathcal{L}_q$ 最终仍会作用到模型权重参数 $\mathcal{W}$ 上，但上述操作解藕了 $f_{\mathcal{W}}$ 和 $\hat{f}_{\mathcal{W, Q}}$ 的优化。如图~\ref{img::gq_nets::detach_grad_cos} 所示，解藕后 $\nabla_{\mathcal{W}} \mathcal{L}_f$ 和 $\nabla_{\mathcal{W}} \mathcal{L}_f$ 在训练过程中基本保持正交，使模型整体训练更快收敛，且最终产生更准确的模型。

\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{GQ_Nets/grad-of-kl-detach/kl_to_ce_angle.pdf}
  \caption{脱离梯度 $\nabla_{\sigma(x_L)}\mathcal{L}_q$ 前后，最后一层权重参数 $W_L$ 收到的梯度间的余弦夹角（左图），及训练过程中 $\FpNet$ 测试准确度（右图）。}
  \label{img::gq_nets::detach_grad_cos}
\end{figure}

上述操作反映到图~\ref{img::gq_nets::arch} 中，即 $\mathcal{L}_q$ 在反向传播过程中，其梯度（对应橙色箭头）仅指向量化输出 $\tilde{x}_L$，而不直接影响 $x_L$。这一操作可以在 PyTorch 中使用 \verb|detach| 操作符，或在 TensorFlow 中使用 \verb|stop_gradient| 操作符实现。其数学定义为
\begin{align}
  \NoDiff{x} =
    \begin{cases}
      x & \text{forward} \\
      0 & \text{backward}
    \end{cases}
\end{align}
在执行上述操作后，\eqref{eq::gq_nets::total_loss} 中 $\mathcal{L}_q$ 项应改写为
\begin{align}
  \mathcal{L}_q = \KL{\sigma(\tilde{x}_L)}{\sigma(\NoDiff{x_L})}
\end{align}
% ------------------------------------------------------------------------------
%    Optimization
% ------------------------------------------------------------------------------
\subsection{损失函数优化} \label{sec::gq_nets::optimize_loss_func}
除量化误差建模及整体损失函数构造外，使用~\eqref{eq::gq_nets::total_loss} 生成的梯度优化模型参数 $\{\mathcal{W}, \Theta\}$ 还需要解决两个问题：$\hat{f}_{\mathcal{W, Q}}$ 计算图反向传播时，不可微分算子及其他特殊算子需做相应处理处理；以及模型训练时对 $\mathcal{L}_f$ 和 $\mathcal{L}_q$ 的侧重程度。

在反向传播时，GQ-Nets 使用 STE 直接将梯度“跳过”计算图中的 $\Round{\cdot}$ 等不可微分操作，即
\begin{align}
  \diff{y}{\Round{x}} \diff{\Round{x}}{x} &= \diff{y}{\Round{x}}
\end{align}
反向传播时的另一类特殊算子，是量化模型 $\hat{f}_{\mathcal{W, Q}}$ 中保留为浮点运行的部分，例如模型的第一层、最后一层等。\footnote{保留浮点操作的量化模型对于实际硬件部署并不友好，讨论此情形主要便于对比其他工作。} 这类层在量化训练时应视为对其量化输入的参数化变换，故其权重参数 $W_i$ 在更新时应只接受由原模型损失函数所传回的梯度 $w_f \nabla_{W_i}\mathcal{L}_f$，而不叠加量化误差函数传回的梯度 $w_q \nabla_{W_i}\mathcal{L}_q$。\improvement{仔细说清楚为什么只接受 $\mathcal{L}_f$ 的梯度}

在 GQ-Nets 训练的不同阶段，损失函数~\eqref{eq::gq_nets::total_loss} 中权重项 $w_f$ 和 $w_q$ 刻画了训练过程中 $\mathcal{L}_f$ 和 $\mathcal{L}_q$ 的相对重要程度。较大的 $w_f$ 会增大由全精度模型损失函数 $\mathcal{L}_f$ 生成的梯度幅度，从而得到准确度较高，却很可能不够量化友好的模型。与之相对，较大的 $w_q$ 会得到量化友好的模型，而其准确度却有可能降低。本章探讨了多种在训练过程中权衡 $w_f$ 和 $w_q$ 的方法。

首先，我们发现用最简单的等值常数权重，即在整个训练过程中 $w_f = w_q = 0.5$ 就可得到优于很多先前方法的结果（见~\ref{sec::gq_nets::exp} 节实验结果）。这可能是因为，由 GQ-Nets 损失函数~\eqref{eq::gq_nets::total_loss} 生成的两组梯度 $w_f \nabla_{\mathcal{W}}\mathcal{L}_f$ 和 $w_q \nabla_{\mathcal{W}, \Theta}\mathcal{L}_q$ 链式传播到 $\mathcal{W}$ 后基本正交而不会相互冲突（见图~\ref{img::gq_nets::detach_grad_cos}）。

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/schedule/schedule.pdf}
    \caption{在训练不同阶段动态调整模型全精度损失项权重 $w_f$ 和量化误差项权重 $w_q$ 的一种策略。在模型训练初始阶段，以及每次学习率下降之后，先不优化量化误差项，之后再缓慢线性增加。}
    \label{img::gq_nets::w_fq_schedule}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/loss_weight/wq_acc_curve.pdf}
    \caption{损失函数权重调整策略 vs. 模型准确度}
    \label{img::gq_nets::schedule_acc}
  \end{subfigure}
  \caption{GQ-Nets 损失函数各项权重动态调整策略，……}
  \label{img::gq_nets::schedule}
\end{figure}

其次，我们根据实验发现，在训练过程中依据特定策略，动态地调整 $w_f$ 和 $w_q$ 能得到准确度更高的量化模型。例如图~\ref{img::gq_nets::w_fq_schedule} 所示的一种策略，在模型训练的整个过程中，$f_{\mathcal{W}}$ 部分训练不作改动，即 $w_f = 1$。同时，在模型训练初始的几轮（即 warmup 阶段，本章使用训练前 $4$ 轮作为 warmup）以及学习率衰减之后的几轮内，由于模型参数分布变化较大、准确度会快速上升，在这些阶段内设置 $w_q = 0$ 以确保量化误差项不干扰训练；在模型训练相对稳定后，再逐步提高 $w_q$ 至 $1$，如图~\ref{img::gq_nets::schedule_acc} 所示。
\improvement{增加使用 GradNorm 之后的权重调整策略}
% ------------------------------------------------------------------------------
%    Multi-domain BN
% ------------------------------------------------------------------------------
\subsection{多域批量正则化}
批量正则化（Batch Normalization，简称 BN）是现代深度神经网络训练中不可或缺的部分。BN 层在训练时通过指数滑动均值（Exponential Moving Average，简称 EMA）统计模型训练时中间各层输出激活的分布信息（各批量激活 $x_{1\ldots L}$ 的均值 $\mu$、方差 $\sigma$），将各激活分布归一化为标准正态分布，从而缓解或消除模型训练时前向传播激活消失、反向传播梯度弥散等问题。在量化神经网络中，一个需要注意的问题是，即使在同一套模型参数 $\{\mathcal{W}, \Theta\}$ 下，全精度模型 $f_{\mathcal{W}}$ 和量化模型 $\hat{f}_{\mathcal{W}, \Theta}$ 的中间激活分布可能很不一样，如图~\ref{img::gq_nets::fp_q_act_dist} 所示。这会导致在 $\FpNet$ 中使用的正则化参数 $\{\mu, \sigma\}$ 无法匹配量化模型 $\QuantNet$ 的激活分布均值 $\hat{\mu}$ 和方差 $\hat{\sigma}$，影响量化模型的训练收敛速率及最终的模型准确度。

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\columnwidth]{GQ_Nets/activation/layer2_0_downsample_0.pdf}
  \caption{全精度模型 $\FpNet$ 和量化模型 $\QuantNet$ 的中间激活分布，……}
  \label{img::gq_nets::fp_q_act_dist}
\end{figure}

在一般的“预训练——量化感知微调”二段式训练过程中，由于在量化感知微调时模型 BN 层仍会通过 EMA 保持更新，其正则化参数 $\{\mu, \sigma\}$ 会逐渐演化为 $\{\hat{\mu}, \hat{\sigma}\}$，从而解决量化模型的激活分布不匹配问题。然而在 GQ-Nets 的训练过程中，全精度模型 $\FpNet$ 和量化模型 $\QuantNet$ 同时存在且共享参数 $\mathcal{W}$，其中包括 $\{\mu, \sigma\}$，故 GQ-Nets 的批量正则化需要做额外处理。受多域适配（Domain Adaptation）领域的工作（……）启发，GQ-Nets 将全精度模型激活 $x_{1\ldots L}$ 和量化模型激活 $\tilde{x}_{1\ldots L}$ 视为来自不同\emph{域}（\emph{domain}）的采样，从而使用两组分离的正则化参数 $\{\mu, \sigma\}$ 和 $\{\hat{\mu}, \hat{\sigma}\}$ 分别对其进行批量正则化操作。在一般神经网络模型，如卷积神经网络、循环神经网络、Transformer 中，BN 层按照逐通道正则化输入激活，即 $\{\mu, \sigma\}$ 通常为维数 $16\sim 512$ 的浮点向量，故维护另一组正则化参数 $\{\hat{\mu}, \hat{\sigma}\}$ 带来的额外存储开销非常少。

在 GQ-Nets 中，我们称维护两组正则化参数以解决全精度模型和量化模型中间激活分布不一致问题的解决方案为\emph{多域批量正则化}（\emph{Multi-domain Batch Normalization}，下文简称 \emph{Multi-domain BN}）。
% ==============================================================================
%  Experiments
% ==============================================================================
\section{主要实验结果} \label{sec::gq_nets::exp}
本节在不同神经网络模型、任务和数据集上实验验证 GQ-Nets 的有效性。为验证 GQ-Nets 对模型结构的普适性，本节使用了在计算机视觉任务中广泛使用且不包含过多冗余参数的模型，包括 ResNets 系列，以及非常紧凑的 MobileNets 系列模型；为验证 GQ-Nets 对于大规模数据的普适性，本节在在物体分类任务中使用 CIFAR-10 和 ILSVRC 2012 数据集验证了 GQ-Nets 的性能。
% 本节在物体分类和物体检测两类任务上验证了 GQ-Nets 的性能。在物体分类任务中，使用了 CIFAR-10 和 ILSVRC 2012 数据集；在物体检测任务中，使用了 MS COCO 数据集。
% ------------------------------------------------------------------------------
%    Experimental settings
% ------------------------------------------------------------------------------
\subsection{实验详细设置} \label{sec::gq_nets::general_conf}
\paragraph{数据集}
本节中讨论的物体分类任务使用了 CIFAR-10 和 ILSVRC 2012 数据集。CIFAR-10~\citep{krizhevsky2009learning} 包含来自 10 个不同类别的、分辨率为 $32 \times 32$ 的图片，其中训练集包含 50K 个样本，测试集包含 10K 个样本，实验结果报告为模型在测试集上的 Top-1 分类准确度。ILSVRC 2012~\citep{ILSVRC15} 是 ImageNet~\citep{deng2009imagenet} 数据集的子集，包含了来自 1K 个类别的图片，其中训练集包含 1.2M 个样本，验证集和测试集分别包含 50K 和 100K 个样本，实验结果报告为模型在验证集上的 Top-1 及 Top-5 准确度。

% 本节中讨论的物体检测任务使用了 MS COCO 数据集及 COCO-2017 Detection Challenge 的标注信息，下文统称 MS COCO。MS COCO~\citep{lin2014microsoft} 数据集包含 115K 训练集样本及 5K 验证集样本，每一样本上标注了来自 80 类物体的检测框（bounding box）。实验结果主要报告为在验证集检测框检测子任务上，模型输出检测框与标注框交并比（intersection over union，下文统称 IoU）阈值为 $[0.5, 0.55, \ldots 0.95]$ 的多类别平均准确度（mean average precision，下文统一记为 $\mathrm{mAP}^{0.5:0.95}$）。
% \unsure{把 COCO 上的实验单独列一节，把这一段移过去？}

\paragraph{主干模型}
本节中使用 ResNet-\{18, 20, 50\}~\citep{He_2016} 和 MobileNet-\{v1, v2\}~\citep{howard2017mobilenets, Sandler_2018} 作为实验的主干模型（backbone network）。在 CIFAR-10 分类任务上，使用了 ResNet-20 模型。注意本章所用的 ResNet-20 与~\citet{zhou2016dorefanet, choi2018pact, li2019additive} 一致，其结构如表~\ref{tab::gq_nets::res20} 所示。

\begin{table}[htb]
  \centering
  \caption{用于 CIFAR-10 数据集的 ResNet-20 模型结构}
  \label{tab::gq_nets::res20}
  \begin{tabular}{l*{3}{c}}
    \toprule
    操作类型 & 操作参数 & 输出通道数 & 输出尺寸 \\
    \midrule
    \emph{输入} & -- & 3 & $32\times32$ \\
    2D 卷积 & 卷积核 $3\times 3$，步长 $1$，补零 $1$ & 16 & $32\times 32$ \\
    基本残差单元 & 单元数 $3$，步长 $1$，补零 $1$ & 16 & $32\times 32$ \\
    基本残差单元 & 单元数 $3$，步长 $2$，补零 $1$ & 32 & $16\times 16$ \\
    基本残差单元 & 单元数 $3$，步长 $2$，补零 $1$ & 64 & $8\times 8$ \\
    2D 均匀池化 & 池化核 $8\times 8$ & 64 & $1\times 1$ \\
    全连接 & -- & 10 & -- \\
    \bottomrule
  \end{tabular}
\end{table}

其中，\emph{基本残差单元}为~\citet{He_2016} 中描述的 basic building block，由两个相同通道数且卷积核尺寸为 $3\times 3$ 的 2D 卷积层及残差连接组成，如图~\ref{img::gq_nets::basic_res_block} 所示；操作参数中\emph{单元数}指模型当前阶段中，堆叠的基本残差单元个数；\emph{步长}和\emph{补零}指残差单元中，第一个 2D 卷积层的卷积核步长及其输入的补零尺寸。在 ILSVRC 2012 物体分类任务上所用其他主干模型配置与~\citet{He_2016,howard2017mobilenets,Sandler_2018} 完全一致。

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\columnwidth]{GQ_Nets/basic_res_block.pdf}
  \caption{ResNet-20（表~\ref{tab::gq_nets::res20}）中所用的基本残差单元结构。图片来自~\citet{He_2016}。}
  \label{img::gq_nets::basic_res_block}
\end{figure}

\paragraph{量化函数}
除非特别说明，本章中所有实验都使用如下量化函数设置：模型权重及激活量化函数均为逐层非对称线性量化，即在量化函数 $Q$ 的定义~\eqref{eq::gq_nets::linear_q_func} 中，模型各层权重及激活张量中所有元素共用当前层的量化宽度 $\Delta$ 和截断边界 $\{lb, ub\}$。模型量化参数 $\Theta$ 中，截断边界 $\{lb, ub\}$ 为可学习参数，在训练过程中随梯度更新。对于权重量化函数 $Q^w_i$，其截断边界分别被初始化为当层权重 $W_i$ 的最小值和最大值；对于激活量化函数 $Q^a_i$，在第一次计算量化误差前，随机从训练集中构造一用于校准的小批量输入（实验中批量尺寸为 $64$），使用该组输入得出模型每层激活值的上下 $99.9\%$ 分位数，并将各层激活量化函数的截断上下边界分别初始化为该校准值。
% ------------------------------------------------------------------------------
%    CIFAR
% ------------------------------------------------------------------------------
\subsection{CIFAR-10 物体分类实验} \label{sec::gq_nets::cifar10_experiments}
所有 CIFAR-10 上的模型使用相同的训练配置。训练在一块 NVIDIA RTX 2080 Ti GPU 上进行，训练批量大小为 128，训练持续 200 轮。模型权重参数 $\mathcal{W}$ 和量化参数 $\Theta$ 使用不同的初始化和训练策略。模型权重参数 $\mathcal{W}$ 以 $\NormalDist{0, \sqrt{\frac{2}{n_{\mathrm{out}}}}}$ 方式初始化，其中 $n_{\mathrm{out}}$ 表示当前参数张量的\emph{扇出（fan-out）}。\footnote{例如对于维度为 $c_{\mathrm{in}} \times c_{\mathrm{out}} \times k_H \times k_W$ 的 2D 卷积核，其扇出计为 $n_{\mathrm{out}}=c_{\mathrm{out}} \times k_H \times k_W$。} 随后使用带 Nesterov 动量的 SGD 算法进行优化。其动量设置为 $0.9$，L2 权重衰减设置为 $10^{-4}$，学习率初始化为 $0.1$，并在训练过程第 80 和第 120 轮分别衰减 $0.1\times$。量化参数 $\Theta$ 按照~\ref{sec::gq_nets::general_conf} 节设定初始化，随后使用 Adam~\citep{kingma2014adam} 算法进行优化。优化过程不使用权重衰减，学习率全程设为 $4\times 10^{-4}$，Adam 算法中第一动量和第二动量分别设为 $\beta_1 = 0.9$ 和 $\beta_2 = 0.999$。训练样本首先通过边缘对称补零拓展至 $40\times40$ 像素，之后随机裁剪至 $32\times 32$ 像素，最终再随机水平翻转并分别在 RGB 通道做归一化；测试样本则只对 RGB 通道分别做归一化。

\begin{table}[htb]
  \centering
  \caption{完全量化的 ResNet-20 GQ-Nets 在 CIFAR-10 物体分类任务上的准确率}
  \label{tab::gq_nets::cifar}
  \begin{tabular}{l *{4}{c}}
    \toprule
    \multirow{2}{*}{主干模型} & \multicolumn{2}{c}{数值精度} &\multicolumn{2}{c}{Top-1 准确率} \\
    & 权重（W） & 激活（A） & 全精度（FP） & 量化（Q） \\
    \midrule
    \multirow{5}{*}{ResNet-20} & 32 & 32 & 91.81 & -- \\
    & 5 & 5 & 91.81 & 91.75 \\
    & 4 & 4 & 91.86 & 91.35 \\
    & 3 & 3 & 91.93 & 90.04 \\
    & 2 & 2 & 91.66 & 84.97 \\
    \bottomrule
  \end{tabular}
\end{table}

模型完全量化时（模型第一层 2D 卷积、最后一层全连接以及模型输入），在 CIFAR-10 测试集上 Top-1 准确率见表~\ref{tab::gq_nets::cifar} 。其中，表格中第一行（W32/A32）为只使用全精度损失函数 $\mathcal{L}_f$ 训练得到的全精度基准模型。可以看出，在实验的所有数值精度设置中，模型全精度准确度几乎都不受影响。在模型参数和激活分别被量化至 W5/A5 和 W4/A4 时，GQ-Nets 在训练后能保持接近全精度模型的准确率（Top-1 准确率相较于 W32/A32 分别 $-0.06\%$ 和 $-0.46\%$）。在模型数值精度被进一步压缩至 W2/A2 后，GQ-Nets 准确率出现了明显的下降（相较于 W32/A32 $-6.84\%$）。

\begin{table}[htb]
  \centering
  \caption{不完全量化的 ResNet-20 GQ-Nets 在 CIFAR-10 物体分类任务上的准确率}
  \label{tab::gq_nets::cifar_fpfl}
  \begin{tabular}{l *{4}{c}}
    \toprule
    \multirow{2}{*}{主干模型} & \multicolumn{2}{c}{数值精度} &\multicolumn{2}{c}{Top-1 准确率} \\
    & 权重（W） & 激活（A） & 全精度（FP） & 量化（Q） \\
    \midrule
    \multirow{5}{*}{ResNet-20} & 32 & 32 & 91.81 & -- \\
    & 5 & 5 & 90.75 & 91.04 \\
    & 4 & 4 & 91.13 & 90.90 \\
    & 3 & 3 & 91.16 & 90.37 \\
    & 2 & 2 & 91.19 & 88.15 \\
    \bottomrule
  \end{tabular}
\end{table}

GQ-Nets 也可以按照模型不完全量化的方式进行量化训练，即模型输入、第一层 2D 卷积和最后一层全连接在训练和推理时保持全精度。该设置与~\citet{zhou2016dorefanet, choi2018pact, Zhang_2018, li2019additive} 一致，该设置下 GQ-Nets 在 CIFAR-10 测试集上的准确率见表~\ref{tab::gq_nets::cifar_fpfl} 。相较于表~\ref{tab::gq_nets::cifar} 中列出的完全量化模型，不完全量化的 GQ-Nets 在低数值精度时准确度改善明显。例如在 W2/A2 设置上，不完全量化模型准确率相较完全量化提升了 $3.18\%$，仅比 W32/A32 模型准确率降低 $3.66\%$。然而，在数值精度相对较高的 W5/A5 和 W4/A4 设置中，不完全量化反而会使模型准确度降低。例如在 W5/A5 设置上，不完全量化的准确度相较完全量化降低了 $0.71\%$。另外值得注意的是，在 W5/A5 $\sim$ W2/A2 的所有量化设置中，不完全量化模型在全精度推理时的准确率均低于完全量化模型。这可能表示，在 GQ-Nets 的训练中，量化误差项 $\mathcal{L}_q$ 的加入可以视为模型训练过程的额外正则化，而完全量化训练的正则化效果更为明显。在 CIFAR-10 这类相对容易的任务中，较强的训练正则化可得到测试准确度更高的模型。

\ctable[
  caption = 不同量化方法的 ResNet-20 在 CIFAR-10 物体分类任务上的准确率,
  pos = htb,
  label = tab::gq_nets::cifar_comparison,
]{l *{5}{c}}{
  \tnote[a]{\citet{zhou2016dorefanet} 没有报告 CIFAR-10 实验结果，数据取自~\citet{choi2018pact} 的复现报告；}
  \tnote[b]{不完全量化，模型输入、第一层和最后一层保留为浮点；}
  \tnote[c]{不完全量化，模型输入、第一层和最后一层数值精度为 W8/A8；}
  \tnote[d]{GQ-Nets 数据第一行表示模型完全量化下的准确率，第二行表示模型不完全量化（即模型输入、第一层和最后一层保留为浮点）下的准确率。}
}{
  \FL
  \multirow{2}{*}{量化方法} & \multicolumn{5}{c}{Top-1 准确率} \NN
  & FP & W5/A5 & W4/A4 & W3/A3 & W2/A2
  \ML
  DoReFa~\citep{zhou2016dorefanet}~\tmark[a]\tmark[b] & 91.6 & 90.4 & 90.5 & 89.9 & 88.2 \NN
  PACT~\citep{choi2018pact}~\tmark[b]        & 91.6 & 91.7 & 91.3 & 91.1 & 89.7 \NN
  LQ-Net~\citep{Zhang_2018}~\tmark[b]        & 92.1 & -- & -- & 91.6 & 90.2 \NN
  APoT~\citep{li2019additive}~\tmark[c]      & 92.96 & -- & 92.45 & 92.49 & 90.96 \NN
  \hdashline
  \multirow{2}{*}{GQ-Nets~\tmark[d]} & \multirow{2}{*}{91.81} & 91.75 & 91.35 & 90.04 & 84.97 \NN
                                                  & & 91.04 & 90.90 & 90.37 & 88.15
  \LL
}

表~\ref{tab::gq_nets::cifar_comparison} 列出了 GQ-Nets 与其他量化训练方法在 CIFAR-10 数据集上的性能对比，这些数据同时包含了模型被完全量化，及模型不完全量化时的准确率。由于 GQ-Nets 的训练策略强调在预训练阶段且不引入过多训练开销，GQ-Nets 不使用预训练参数且训练流程不超过全精度模型预训练轮数，故不同数值精度下完全量化的 GQ-Nets 的模型准确度与不完全量化的 \citet{zhou2016dorefanet} 相当，与领域最优的 \citet{Zhang_2018, li2019additive} 差距明显。但是正如将在第~\ref{sec::gq_nets::other_training_protocol} 节所讨论的，如果不在意引入更多训练预算，GQ-Nets 模型准确度可以达到或超过上述领域内现有最优方法。
% ------------------------------------------------------------------------------
%    ILSVRC
% ------------------------------------------------------------------------------
\subsection{ILSVRC 2012 物体分类实验}
由于数据量显著增大，GQ-Nets 在 ILSVRC 2012 数据集上的实验在 32 块 NVIDIA GTX 1080 Ti GPU 上使用数据并行模式进行。每一节点的训练批量大小为 64，每一步反向传播完成后，各节点间使用 all-reduce 同步梯度并更新模型参数，训练持续 120 轮。模型权重参数 $\mathcal{W}$ 以 Kaiming-normal~\citep{He_2015} 方式初始化。由于全局训练批量尺寸增大，优化 $\mathcal{W}$ 的学习率按照~\citet{goyal2017accurate} 提出的 \emph{warmup} 策略，在训练开始的前 4 轮从 0.2 线性增大至 0.8，并分别在训练的第 60 轮和第 90 轮衰减 $\times 0.1$。训练样本首先被随机裁剪至 $224\times 224$ 像素并随机水平翻转，之后在 RGB 通道分别归一化；测试样本先缩放至图片短边 256 像素，之后在缩放后的图片中央裁剪 $224\times 224$ 像素区域，最后在 RGB 通道分别做归一化。其他实验配置与 CIFAR-10 上的实验配置（第~\ref{sec::gq_nets::cifar10_experiments} 节）一致。

\ctable[
  caption = 完全量化的 GQ-Nets 在 ILSVRC 2012 物体分类任务上的准确率,
  pos = p,
  label = tab::gq_nets::imgnet,
]{l *{5}{c}}{
  \tnote[a]{由于显存限制，QAT 阶段单 GPU 训练批量大小为 32，学习率对应减小一半；}
  \tnote[b]{QAT 1 轮之后出现量化范围交错；}
  \tnote[c]{模型实现及全精度参数取自 \verb|torchvision (v0.5.0)|：\url{https://pytorch.org/docs/stable/torchvision/models.html}；}
  \tnote[d]{模型实现取自：\url{https://github.com/marvis/pytorch-mobilenet}；}
}{
  \FL
  \multirow{2}{*}{主干模型} & \multirow{2}{*}{数值精度（W/A）} &\multicolumn{2}{c}{Top-1 准确率} &\multicolumn{2}{c}{Top-5 准确率} \\
  & & 全精度 & 量化 & 全精度 & 量化
  \ML
  \multirow{5}{*}{ResNet-18} & 32/32~\tmark[c] & 69.89 & -- & 89.27 & -- \\
  & 5/5 & 69.63 & 68.86 & 89.16 & 88.62 \\
  & 4/4 & 69.12 & 67.32 & 88.89 & 87.73 \\
  & 3/3 & 68.81 & 64.16 & 88.81 & 85.91 \\
  & 2/2 & 67.56 & 52.57 & 88.06 & 77.13 \\
  \hdashline
  \multirow{5}{*}{ResNet-34} & 32/32~\tmark[c] & 73.30 & -- & 91.42 & -- \\
  & 5/5 & 72.94 & 72.40 & 91.13 & 90.81 \\
  & 4/4 & 73.07 & 71.53 & 91.33 & 90.46 \\
  & 3/3 & 72.58 & 68.45 & 90.95 & 88.36 \\
  & 2/2 & 71.01 & 59.13 & 90.01 & 82.15 \\
  \hdashline
  \multirow{5}{*}{ResNet-50~\tmark[a]} & 32/32~\tmark[c] & 76.15 & -- & 92.87 & -- \\
  & 5/5 & 75.16 & 74.69 & 92.45 & 82.18 \\
  & 4/4 & 75.44 & 74.35 & 92.51 & 91.81 \\
  & 3/3 & 72.58 & 68.45 & 90.95 & 88.36 \\
  & 2/2~\tmark[b] & -- & -- & -- & -- \\
  \hdashline
  MobileNet-v1 & 32/32~\tmark[d] & 70.68 & -- & 89.84 & -- \\
  & 4/4 & 68.31 & 65.04 & 88.47 & 86.09 \\
  \hdashline
  MobileNet-v2 & 32/32~\tmark[c] & 71.09 & -- & 90.12 & -- \\
  & 4/4 & 68.58 & 66.15 & 88.70 & 86.92
  \LL
}

使用不同主干模型和量化精度的实验结果见表~\ref{tab::gq_nets::imgnet} 所示。在所有主干模型所列结果的第 1 行为其全精度下准确度，除 MobileNet-v1 模型实现来自 GitHub 第三方实现外，其他模型实现及全精度参数来自 \verb|torchvision| 软件包。在模型完全量化的情况下，使用 ResNet 系列主干模型的 GQ-Nets 可在 W4/A4 及以上数值精度下保留可接受的准确度：在 W4/A4 数值精度下，ResNet-50、-34、-18 模型的量化输出 Top-1 准确率相较其全精度基线仅下降 $-1.80\%$、 $-1.77\%$ 和 $-2.57\%$；在 W5/A5 数值精度下，Top-1准确率相较全精度基线下降 $-1.46\%$、$-0.90\%$ 和 $-1.03\%$。考虑到 GQ-Nets 训练策略并不使用预训练模型，即量化模型的累计训练轮数与全精度基线是一致的，且 GQ-Nets 模型量化了部署时的所有参数及数值运算操作，这样的准确度损失相较于部署全量化模型带来的加速优势，是可以接受的。

值得注意的是，本身已经足够紧凑的 MobileNet 系列模型在 GQ-Nets 框架下仍能在完全量化后保持可接受的准确率。在 W4/A4 数值精度下，MobileNet-v1、-v2 模型量化输出 Top-1 准确率相较于全精度基线下降分别为 $-5.64\%$ 和 $-4.94\%$。

另外需要注意的是，极低数值精度下 GQ-Nets 在较困难的大规模数据集上，会出现较小规模数据集上更明显的准确度损失。例如在 W2/A2 数值精度下对比表~\ref{tab::gq_nets::imgnet} 和表~\ref{tab::gq_nets::cifar_comparison} 上的结果，会发现在 ILSVRC 2012 上训练的 ResNet 系列模型 Top-1 准确度损失在 $-14.17\% \sim -17.32\%$ 之间，而 CIFAR-10 上训练的 ResNet-20 准确度损失为 $-6.78\%$。同时，参数量较少的模型在量化后也会带来更多的准确度损失，例如 ResNet-18 模型量化后的准确率下降在各数值精度下均高于 ResNet-50。

\begin{table}[htb]
  \caption{Comparison with other strict quantization methods on top-1 and top-5 accuracy for ImageNet. W/A indicates the quantization bitwidths for weights and activations, respectively. 32/32 indicates full-precision models.}
  \label{tab::gq_nets::imgnet_comparison}
  \begin{center}
    \begin{tabular}{l *{7}{c}}
      \toprule
      \multirow{2}{*}{量化方法} & \multirow{2}{*}{W/A} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{MobileNet-v1} & \multicolumn{2}{c}{MobileNet-v2} \\
      & & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\
      \midrule
      \emph{全精度基线} & 32/32 & 69.89 & 89.27 & 70.68 & 89.64 & 71.09 & 90.12 \\
      \hdashline
      \multirow{2}{*}{\citet{krishnamoorthi2018quantizing}} & 4/8 & & & 65.00 & & 62.00 & \\
        & 8/4 & & & 64.00 & & 58.00 & \\
      \hdashline
      \citet{jacob2018quantization} & 5/5 & 64.64 & 86.67 & & & & \\
      \hdashline
      \multirow{2}{*}{\citet{louizos2018relaxed}} & 5/5 & 65.10 & 86.57 & 61.38 & 83.73 & & \\
        & 4/4 & 61.52 & 83.99 & & & & \\
      \hdashline
      % \multirow{2}{*}{GQ-Nets}  & 5/5 & 68.86 & 88.62 & & & & \\
        GQ-Nets & 4/4 & 67.32 & 87.73 & 65.04 & 86.09 & 66.15 & 86.92 \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

由于 GQ-Nets 主要关注模型完全量化后的性能表现，故在与其他方法对比时，对比基线选取为同样对模型做完全量化的 \citet{jacob2018quantization, krishnamoorthi2018quantizing, louizos2018relaxed}。在 ILSVRC 2012 数据集上的对比结果见表~\ref{tab::gq_nets::imgnet_comparison} 所示。在模型压缩领域广受关注的 ResNet-18 模型上，4-bit GQ-Nets 的量化 Top-1 准确度优于先前最优的完全量化方法 \citet{louizos2018relaxed} 在 5-bit 和 4-bit 上的准确度——在 W4/A4 数值精度下，GQ-Nets 仅损失 $-2.57\%$ 准确度，而 \citet{louizos2018relaxed} 在 W5/A5 和 W4/A4 下分别损失了 $-4.79\%$ 和 $-8.37\%$ 准确度。在广泛部署至低性能终端的 MobileNet 系列模型上，GQ-Nets 相较与 \citet{jacob2018quantization, krishnamoorthi2018quantizing} 在更低数值精度下取得了近似或更高准确度：在 MobileNet-v1 模型上，W4/A4 精度的 GQ-Nets 准确度较 W4/A8 精度的 \citet{krishnamoorthi2018quantizing} 高 $+0.04\%$（$65.04\%$ Top-1 准确度对比 $65.00\%$ Top-1 准确度）；在 MobileNet-v2 模型上，W4/A4 精度的 GQ-Nets 模型较 W4/A8 的 \citet{krishnamoorthi2018quantizing} 准确度提高了 $+4.15\%$（$66.15\%$ Top-1 准确度对比 $62.00\%$ Top-1 准确度）。
% ------------------------------------------------------------------------------
%    COCO
% ------------------------------------------------------------------------------
% \subsection{MS COCO 物体检测实验}
% TODO
% ==============================================================================
%  Analysis and discussions
% ==============================================================================
\section{分析实验及讨论}
由于第~\ref{sec::gq_nets::exp} 节报告了使用不同主干模型、数值精度的 GQ-Nets 在不同任务和数据集上的表现，同时 GQ-Nets 在算法设计和训练策略中包含了多个元素，故本节对 GQ-Nets 进行更深入的分析讨论。本节首先讨论不同数值精度下 GQ-Nets 训练过程的收敛情况，之后通过模型简化实验（ablation study）确认 GQ-Nets 算法各模块对最终性能的贡献，最后讨论在训练预算更充裕时，可以显著改进 GQ-Nets 任务准确率的其他训练策略。
% ------------------------------------------------------------------------------
%    Convergence
% ------------------------------------------------------------------------------
\subsection{收敛性分析}

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/convergence/res20_train_fp_acc.pdf}
    \caption{全精度输出训练准确度}
    \label{img::gq_nets::res20_train_fp_acc}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/convergence/res20_val_fp_acc.pdf}
    \caption{全精度输出测试准确度}
    \label{img::gq_nets::res20_eval_fp_acc}
  \end{subfigure}
  \newline
  \vspace*{0.5 cm}
  \newline
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/convergence/res20_train_quant_acc.pdf}
    \caption{量化输出训练准确度}
    \label{img::gq_nets::res20_train_quant_acc}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/convergence/res20_val_quant_acc.pdf}
    \caption{量化输出测试准确度}
    \label{img::gq_nets::res20_eval_quant_acc}
  \end{subfigure}
  \caption{使用 ResNet-20 主干网络的 GQ-Nets 在 CIFAR-10 数据集上的训练收敛情况。}
  \label{img::gq_nets::res20_convergence}
\end{figure}

本节通过观察不同数值精度的 GQ-Nets 模型在 CIFAR-10 数据集上训练过程中的训练/测试准确度，以及其中主要量化参数的分布变化，来获知并探讨 GQ-Nets 的收敛性特征。图~\ref{img::gq_nets::res20_convergence} 展示了使用 ResNet-20 主干模型的 GQ-Nets 在训练过程中其全精度输出 $\sigma(x_L)$ 和量化输出 $\sigma(\tilde{x}_L)$ 在训练过程中在 CIFAR-10 训练/测试集上的准确度。如图~\ref{img::gq_nets::res20_train_fp_acc} 及图~\ref{img::gq_nets::res20_eval_fp_acc} 所示，不同数值精度下 GQ-Nets 的全精度分支几乎表现一致，即引入量化损失几乎不对模型的原有训练过程产生负面影响。这进一步验证了第~\ref{sec::gq_nets::optimize_loss_func} 节及图~\ref{img::gq_nets::detach_grad_cos} 中声明的，GQ-Nets 中减少量化损失产生的梯度与原模型训练梯度基本正交的观点。

需要注意的是，如图~\ref{img::gq_nets::res20_train_quant_acc} 及图~\ref{img::gq_nets::res20_eval_quant_acc} 所示，不同量化精度的 GQ-Nets 量化分支收敛速率和最终准确度可能会呈现明显差别。在 CIFAR-10 数据集上，W5/A5 和 W4/A4 数值精度的收敛速率和最终准确率没有明显差别，说明在相对简单的任务上 4-bit 数值精度即可达到模型完全量化后保持模型性能的要求；W2/A2 数值精度在训练时的收敛速率与其他设置有明显差距，且其在测试集上的准确率也会在训练过程中出现较大波动，说明极低比特量化与一般量化在训练时存在明显差别，其训练策略需要更仔细考虑。

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/convergence/layer3.2.conv2_act_bounds.pdf}
    \caption{QAT 过程中激活量化边界}
    \label{img::gq_nets::act_bounds}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{GQ_Nets/convergence/layer3.2.conv2_weight_bounds.pdf}
    \caption{QAT 过程中模型参数量化边界}
    \label{img::gq_nets::weight_bounds}
  \end{subfigure}
  \caption{ResNet-20 GQ-Nets 模型在 CIFAR-10 数据集上 QAT 过程中模型 \texttt{layer3.2.conv2} 层量化参数收敛情况。}
  \label{img::gq_nets::res20_quant_bounds}
\end{figure}

图~\ref{img::gq_nets::res20_quant_bounds} 展示了使用 ResNet-20 主干网络的 GQ-Nets 中 \verb|layer3.2.conv2| 层（主干网络最后一个 2D 卷积层）在训练过程中，其模型参数和输出激活的量化边界在不同量化精度下的收敛情况。随着量化数值精度的降低，模型参数和激活的量化范围都在逐渐变窄，说明可用量化点较低的极低比特量化的模型在训练过程中会试图通过降低量化范围来提高量化点间的分辨率。同时需要注意模型参数量化和激活量化在训练过程中呈现了不同的趋势：图~\ref{img::gq_nets::act_bounds} 说明模型激活在训练初期首先逐渐增大量化范围，在学习率衰减后量化范围开始收窄，并在模型测试准确率稳定后同样趋于稳定；而图~\ref{img::gq_nets::weight_bounds} 说明模型参数在训练开始后就迅速趋于缩减分布数值范围，且量化数值精度越低，其收敛的速率越显著。
% ------------------------------------------------------------------------------
%    Ablation
% ------------------------------------------------------------------------------
\subsection{模型简化分析}

\begin{table}[htb]
  \centering
  \caption{使用 ResNet-18 主干模型，量化数值精度为 W4/A4 的的 GQ-Nets 在 ILSVRC 2012 数据集上的模型简化实验。第~\ref{sec::gq_nets::train} 节中改进 GQ-Nets 损失函数及训练过程的算法模块被依次加入到图~\ref{img::gq_nets::arch} 所示的 GQ-Nets 基本架构中，以验证各改进对最终性能的贡献。表格第 1 行指不含任何改进的 GQ-Nets 在 W4/A4 数值精度下训练完成后，其全精度和量化输出的 Top-1 准确度；包含 Alt. $\{\mathcal{W}, \Theta\}$ 的实验结果指模型训练时交替优化模型权重参数 $\mathcal{W}$ 和量化参数 $\Theta$；包含 Sch. $w_f, w_q$ 的实验结果指训练时按照图~\ref{img::gq_nets::schedule} 所示策略动态调整模型原损失函数 $\mathcal{L}_f$ 和量化损失函数 $\mathcal{L}_q$ 的权重；包含 Detach $\sigma(x_L)$ 的实验结果指模型损失函数按照第~\ref{sec::gq_nets::loss_func} 节讨论，不回传梯度 $\nabla_{\sigma(x_L)} \mathcal{L}_q$；包含 MD-BN 的实验结果指模型在 BN 操作中，对全精度激活和量化激活使用不同的统计值 $\{\mu, \sigma\}$ 和 $\{\hat{\mu}, \hat{\sigma}\}$。模型在全精度下的基准 Top-1 准确度为 69.89\%。}
  \label{tab::gq_nets::ablation}
  \begin{tabular}{*{4}{c} *{2}{c}}
    \toprule
    Alt. $\{\mathcal{W}, \Theta\}$ & Sch. $w_f, w_q$ & Detach $\sigma(x_L)$ & MD-BN & Top-1 (FP) & Top-1 (Q) \\
    \midrule
      & & & & 65.21 & 60.95 \\
    \checkmark & & & & 67.14 & 65.19 \\
    \checkmark & \checkmark & & & 66.56 & 65.42 \\
    \checkmark & \checkmark & \checkmark & & 66.61 & 66.08 \\
    \checkmark & \checkmark & \checkmark & \checkmark & \textbf{68.30} & \textbf{66.68} \\
    \bottomrule
  \end{tabular}
\end{table}

本节在 ILSVRC 2012 数据集上，对使用 ResNet-18 主干模型且量化数值精度为 W4/A4 的 GQ-Nets 进行模型简化分析，结果见表~\ref{tab::gq_nets::ablation}。在不使用任何针对损失函数和训练过程的改进的情况下，如图~\ref{img::gq_nets::arch} 所示的基本 GQ-Nets 量化准确度为 $60.95\%$。具体地，该基线模型在训练每一步同时优化模型权重参数 $\mathcal{W}$ 和量化参数 $\Theta$；在训练全程为损失函数各项赋予相等权重 $w_f = w_q = 0.5$；反向传播时不额外修改计算图，量化损失项 $\mathcal{L}_q$ 的梯度同时反向传播至模型全精度输出 $\sigma(x_L)$ 和量化输出 $\sigma(\tilde{x}_L)$；所有 BN 层对量化激活和全精度激活使用同一套正则化统计参数 $\{\mu, \sigma\}$。

首先讨论训练过程中交替优化模型权重参数 $\mathcal{W}$ 和量化参数 $\Theta$ 的贡献。对比表~\ref{tab::gq_nets::ablation} 第 1 行和第 2 行，交替优化后模型在基线上提高了 $+4.24\%$ 量化准确度。这说明在基线模型中 $\{\mathcal{W}, \Theta\}$ 的优化可能是互相耦合的，即优化方向 $\nabla_{\mathcal{W}} \mathcal{L}$ 和 $\nabla_{\Theta} \mathcal{L}$ 都分别能够提升模型的准确度，但是合并这两个优化方向可能导致次优的最终结果。

对比表~\ref{tab::gq_nets::ablation} 第 2 行和第 3 行可以看出，在训练过程中动态调整全精度损失项权重 $w_f$ 和量化损失项权重 $w_q$ 在交替优化的基础上提高了 $+0.23\%$ 准确度。这说明 GQ-Nets 在训练过程中全精度损失和量化损失之间最优重要性并不是恒定的，使用适当的方式对损失权重做动态调整可以改善模型的最终性能。

通过不回传量化损失对于全精度输出的梯度 $\nabla_{\sigma(x_L)} \mathcal{L}_q$ 可以再提高 $+0.66\%$ 量化准确度。这说明梯度 $\nabla_{\sigma(x_L)} \mathcal{L}_q$ 是导致全精度模型优化和量化模型优化耦合的原因之一，脱离此梯度可以一定程度解藕优化过程，改善量化模型的准确度。

最后，模型 BN 层对量化激活和全精度激活使用不同的正则化统计参数带来了 $+0.60\%$ 的准确度提升。这说明如图~\ref{img::gq_nets::fp_q_act_dist} 所示的量化/全精度激活分布不一致会影响模型训练，且用同一套统计参数不足以同时对这两类激活做正则化。由于 GQ-Nets 框架内即同时包含了量化及全精度分支，故需要对不同分支分别采用不同的正则化统计参数。
% ------------------------------------------------------------------------------
%    Training time and extension training
% ------------------------------------------------------------------------------
\subsection{其他训练策略} \label{sec::gq_nets::other_training_protocol}

\ctable[
  caption = 其他训练策略在 CIFAR-10 上的准确度、训练总时长比较。,
  pos = htb,
  label = tab::gq_nets::other_training_protocol,
]{l *{5}{c}}{
  \tnote[a]{时间格式：\verb|<小时: 分钟: 秒>|；}
  \tnote[b]{不完全量化，模型输入、第一层和最后一层保留为浮点；}
  \tnote[c]{模型训练在 NVIDIA GTX 1080Ti GPU 上进行；}
  \tnote[d]{模型训练在 NVIDIA RTX 2080Ti GPU 上进行；}
}{
  \FL
  \multirow{2}{*}{训练策略} & \multicolumn{2}{c}{训练开销} & 数值精度 &\multicolumn{2}{c}{Top-1 准确率} \\
  & 累计轮数 & 累计用时~\tmark[a] &（W/A）& 全精度 & 量化
  \ML
  \emph{预训练参数} & 200 & 1:9:16~\tmark[d] & 32/32 & 91.81 & -- \\
  \hdashline
  \multirow{2}{*}{GQ-Nets} & 200 & 2:4:22~\tmark[d] & 2/2 & 91.66 & 84.97 \\
    & 200 & 1:53:48~\tmark[d] & 2/2~\tmark[b] & 91.19 & 88.15 \\
  \hdashline
  \multirow{2}{*}{GQ-Nets + 预训练参数} & 400 & 3:12:25~\tmark[c] & 2/2 & 92.02 & 85.32 \\
    & 400 & 3:04:04~\tmark[d] & 2/2~\tmark[b] & 91.96 & 90.01 \\
  \hdashline
  \multirow{4}{*}{GQ-Nets + 渐进训练} & 400 & 3:12:56~\tmark[c] & 5/5 & 92.17 & 92.20  \\
    & 600 & 5:16:10~\tmark[c] & 4/4 & 92.70 & 92.13  \\
    & 800 & 7:18:39~\tmark[c] & 3/3 & 92.85 & 90.89  \\
    & 1000 & 9:20:51~\tmark[c] & 2/2 & 92.99 & 85.99  \\
  \hdashline
  \multirow{4}{*}{GQ-Nets + 渐进训练~\tmark[b]} & 400 & 3:17:52~\tmark[d] & 5/5 & 92.30 & 92.47 \\
    & 600 & 5:10:59~\tmark[d] & 4/4 & 92.33 & 92.44 \\
    & 800 & 7:16:50~\tmark[d] & 3/3 & 92.84 & 92.19 \\
    & 1000 & 9:09:17~\tmark[d] & 2/2 & 92.50 & 90.18 
  \LL
}

从表~\ref{tab::gq_nets::cifar_comparison} 和表~\ref{tab::gq_nets::imgnet_comparison} 的数据可以得知，由于 GQ-Nets 强调在模型预训练阶段，以不高于原模型训练轮数的开销得出量化友好模型，导致了 GQ-Nets 与其他注重量化模型准确率而不关注训练开销的方法~\citep{jung2019learning, li2019additive} 尚有明显量化准确度差距。本节探讨若不关注训练开销，可以改进 GQ-Nets 量化准确率的其他训练策略，并在 CIFAR-10 数据集和 ResNet-20 模型上比较这些策略与第~\ref{sec::gq_nets::train} 节讨论的训练策略的实际训练开销及模型最终准确率比较。

领域内基于 QAT 的量化模型训练方法一般会使用全精度\emph{预训练参数}，即先将模型以全精度在任务数据上训练至收敛，再使用收敛后的参数进行 QAT 得到量化准确率足够高的模型。本节验证了使用预训练参数的 GQ-Nets 的训练策略，具体地，我们先将全精度模型按照第~\ref{sec::gq_nets::cifar10_experiments} 节训练策略在 CIFAR-10 训练集上训练 200 轮至收敛，之后以 W2/A2 量化精度重复上述训练流程，并报告模型最终准确率和累计训练时长。下文称此训练策略为\emph{GQ-Nets + 预训练参数}。

在训练极低比特的量化模型时，\citet{li2019additive} 使用了训练开销较大但最终准确率改善明显的训练策略：例如在训练 W2/A2 数值精度的极低比特模型时，先使用全精度预训练参数初始化 W5/A5 数值精度的普通量化模型，待训练收敛后再以 W5/A5 模型参数初始化 W4/A4 数值精度的模型并以相同流程训练，以此渐进地降低训练过程中模型量化数值精度，直至得到训练收敛的 W2/A2 模型为止。本节称这种训练策略为\emph{渐进训练}，并将此策略应用至 GQ-Nets 的训练过程中。下文称此训练策略为\emph{GQ-Nets + 渐进训练}。

将上述不考虑训练开销的训练策略应用至 GQ-Nets 后的结果见表~\ref{tab::gq_nets::other_training_protocol} 所示，其中列出了不同训练策略达到目标量化精度所用的累计训练轮数、累计训练用时及最终的模型准确度。表中也同时列出了模型量化与不完全量化的 GQ-Nets 在这些训练策略下的开销与性能。使用预训练参数在完全量化的 GQ-Nets 上改善了 $+0.35\%$ 准确度，在不完全量化的 GQ-Nets 上改善了 $+1.86\%$ 准确度，但代价是训练轮数从 200 轮增至 400 轮。使用渐进训练可明显改善极低比特量化模型的准确率：例如在 W2/A2 量化精度下，使用渐进训练策略可以使完全量化的 GQ-Nets 准确率提高 $+1.02\%$，不完全量化的 GQ-Nets 准确率提高 $+2.03\%$，从而达到或超过先前准确率最高的~\citet{Zhang_2018, li2019additive} 的准确率水平。但是代价是训练轮数达到了 GQ-Nets 训练策略的 5 倍，即使在 NVIDIA RTX 2080Ti GPU 上仍需要约 9 小时才能得到在 CIFAR-10测试集上，以 W2/A2 量化精度达到 $90.18\%$ 准确率的模型。
% ==============================================================================
%  Conclusion
% ==============================================================================
\section{结论}
TODO

