\chapter{量化网络的反向蒸馏训练} \label{chap::gq-nets}

% ==============================================================================
%  Introduction
% ==============================================================================
\section{引言}
第~\ref{chap::fqn} 章针对 FQN 模型的分析显示，基于量化神经网络的目标检测模型训练较一般模型困难，其原因在于量化误差为目标检测模型训练引入了更多不稳定性。若针对此类误差建模，并在训练过程中显式地优化该误差，则有可能得到更为鲁棒的量化目标检测模型。

本章介绍一种将神经网络量化误差以可微分形式隐式建模，并利用类似模型蒸馏训练的反向蒸馏技术，将该误差模型引入原网络训练过程的训练方法。该方法可平行整合至原模型训练方法中，将模型训练过程引导至同时最小化原损失函数和量化误差的求解点，从而得到适合被量化运行的神经网络模型。我们称使用该方法训练的量化神经网络为 GQ-Nets (Guided Quantization Networks)。

GQ-Nets 使用在相同输入下，量化神经网络和全精度浮点神经网络之间最终输出的不一致性作为量化误差的度量。在分类任务中，该不一致性由各神经网络最后一层输出（即 logits）间的 KL--散度刻画；在检测任务中，则由模型分类子网络输出的 logits 间的 KL--散度，以及回归子网络输出间的 smooth L1-loss \unsure{也许应该使用 L2-loss？} 一同刻画。这类一致性标准虽然没有针对模型参数及量化参数的显式表达式，但通过在量化操作的反向传播过程中引入 STE，其计算过程变得可微分，从而使得量化误差能够在训练过程中被显式地优化。

在将针对量化误差的优化引入原模型训练过程后，原模型优化问题演化为一阶多目标优化问题，即需要考虑最小化量化误差的梯度是否会与原模型训练梯度冲突。通过在模型训练过程中对这两组梯度间余弦相似度的度量，揭示了这两组梯度在训练的多数过程中近似正交；即对于大部分任务而言，减少模型量化误差这一任务并不与模型原始任务冲突。基于上述观察，本章引入了调和这两组梯度的一系列训练方法，并对其效果进行了分析比较。
% ------------------------------------------------------------------------------
%    Motivation
% ------------------------------------------------------------------------------
\subsection{工作动机}

\paragraph{对量化误差的端到端建模}
之前工作对神经网络量化过程的误差建模多集中在逐层或逐张量误差分析，以及依赖特定显式一致性指标，例如（……）。然而，在较为复杂的神经网络中，这可能导致两方面的问题：
\begin{enumerate}[1)]
  \item 逐层或逐张量误差分析可能不够准确：例如在 MobileNet 系列模型中，由于层参数量较少导致各卷积核表达能力受限，使用 depth-wise convolution 的中间层会产生较大的参数量化误差和激活量化误差，然而模型最终的误差却仍可能被控制在较小范围内（……）；
  \item 使用特定的显式一致性指标也不一定能适应各种任务：例如在目标检测的回归子网络中，其输出的一致性指标一般用 L2-loss 或 smooth L1-loss 刻画；在嵌入向量的非监督学习中，则一般用余弦相似度刻画嵌入向量间的一致性；
\end{enumerate}
因此我们提出，应针对所用神经网络模型的任务使用相应的一致性指标，并以\textit{相同输入}下，全精度模型和量化模型\textit{最终输出}间的一致性，作为量化误差的衡量标准。也就是说，相较于模型训练或推理时某些中间层的局部误差，我们更关注模型最终输出间的不匹配度。

\paragraph{在模型预训练阶段即引入量化误差}
之前基于量化感知训练的工作，一般采取``预训练--量化感知微调''的两段式训练方法来减少神经网络的量化误差。然而，这类训练流程的代价是训练时间消耗几乎是传统训练的 $2\times$ 以上，且在量化感知微调阶段需要模型的完整训练数据集。例如在 LIQ 的 ImageNet 分类任务上，模型先在预训练阶段训练 120 轮，之后再在量化感知微调阶段训练 $60\sim 90$ 轮。如果考虑在量化感知微调阶段，模型计算图中包含的更多额外操作（例如量化、反量化、量化 STE 算子的梯度修改等），则在 ImageNet 分类任务上 LIQ 的总训练时长约是一般全精度模型的 $2\times$ 左右。与此同时，在实际应用场景中，模型在量化部署前一般很难接触到完整的训练数据集（例如模型的设计、训练和部署分别由不同的技术团队负责，各团队间数据不一定共享），这直接导致了两段式量化感知训练在实际场景中不一定可行。因此在 GQ-Nets 在能够接触完整训练数据的模型预训练阶段，即开始考虑模型量化运行的误差，并直接在模型参数优化时，尽可能消除该误差。

\paragraph{调和训练阶段多任务梯度}
对于较复杂的神经网络模型而言，其训练过程已包含来自不同任务损失函数的梯度——例如在用于实例分割的 Mask R-CNN 训练过程中，模型参数同时被最小化分类损失、检测锚框回归损失和分割掩模的三组梯度更新。在 GQ-Nets 的训练过程中，最小化量化误差会为模型参数生成另一组梯度。使用多组随训练过程频繁变化的梯度，优化同一组模型参数是较为困难的。因此，GQ-Nets 提出的在预训练阶段减小量化误差的挑战之一是，调和各损失函数的对应梯度，使模型能够收敛至同时最小化所有损失函数的解。
% ------------------------------------------------------------------------------
%    Contributions
% ------------------------------------------------------------------------------
\subsection{主要贡献}
TODO

% ==============================================================================
%  Modeling Quantization Errors
% ==============================================================================
\section{量化误差的建模}
TODO

% ==============================================================================
%  Method
% ==============================================================================
\section{反向蒸馏训练}
TODO
% ------------------------------------------------------------------------------
%    Loss function
% ------------------------------------------------------------------------------
\subsection{损失函数设计}
TODO
% ------------------------------------------------------------------------------
%    Optimization
% ------------------------------------------------------------------------------
\subsection{损失函数优化}
TODO
% ------------------------------------------------------------------------------
%    Multi-domain BN
% ------------------------------------------------------------------------------
\subsection{多域批量正则化}
TODO

% ==============================================================================
%  Experiments
% ==============================================================================
\section{主要实验结果}
TODO

% ==============================================================================
%  Ablation study
% ==============================================================================
\section{对比实验及分析}
TODO

% ==============================================================================
%  Conclusion
% ==============================================================================
\section{结论}
TODO

