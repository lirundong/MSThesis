\chapter{量化网络的反向蒸馏训练} \label{chap::gq_nets}

% ==============================================================================
%  Introduction
% ==============================================================================
\section{引言}
第~\ref{chap::fqn} 章针对 FQN 模型的分析显示，基于量化神经网络的目标检测模型训练较一般模型困难，其原因在于量化误差为目标检测模型训练引入了更多不稳定性。若针对此类误差建模，并在训练过程中显式地优化该误差，则有可能得到更为鲁棒的量化目标检测模型。

本章介绍一种将神经网络量化误差以可微分形式隐式建模，并利用类似模型蒸馏训练的反向蒸馏技术，将该误差模型引入原网络训练过程的训练方法。该方法可平行整合至原模型训练方法中，将模型训练过程引导至同时最小化原损失函数和量化误差的求解点，从而得到适合被量化运行的神经网络模型。我们称使用该方法训练的量化神经网络为 GQ-Nets (Guided Quantization Networks)。

GQ-Nets 使用在相同输入下，量化神经网络和全精度浮点神经网络之间最终输出的不一致性作为量化误差的度量。在分类任务中，该不一致性由各神经网络最后一层输出（即 logits）间的 KL--散度刻画；在检测任务中，则由模型分类子网络输出的 logits 间的 KL--散度，以及回归子网络输出间的 smooth L1-loss \unsure{也许应该使用 L2-loss？} 一同刻画。这类一致性标准虽然没有针对模型参数及量化参数的显式表达式，但通过在量化操作的反向传播过程中引入 STE，其计算过程变得可微分，从而使得量化误差能够在训练过程中被显式地优化。

在将针对量化误差的优化引入原模型训练过程后，原模型优化问题演化为一阶多目标优化问题，即需要考虑最小化量化误差的梯度是否会与原模型训练梯度冲突。通过在模型训练过程中对这两组梯度间余弦相似度的度量，揭示了这两组梯度在训练的多数过程中近似正交；即对于大部分任务而言，减少模型量化误差这一任务并不与模型原始任务冲突。基于上述观察，本章引入了调和这两组梯度的一系列训练方法，并对其效果进行了分析比较。
% ------------------------------------------------------------------------------
%    Motivation
% ------------------------------------------------------------------------------
\subsection{工作动机}

\paragraph{对量化误差的端到端建模}
之前工作对神经网络量化过程的误差建模多集中在逐层或逐张量误差分析，以及依赖特定显式一致性指标，例如（……）。然而，在较为复杂的神经网络中，这可能导致两方面的问题：
\begin{enumerate}[1)]
  \item 逐层或逐张量误差分析可能不够准确：例如在 MobileNet 系列模型中，由于层参数量较少导致各卷积核表达能力受限，使用 depth-wise convolution 的中间层会产生较大的参数量化误差和激活量化误差，然而模型最终的误差却仍可能被控制在较小范围内（……）；
  \item 使用某一特定的显式一致性指标不一定能适应各种任务：例如在目标检测的回归子网络中，其输出的一致性指标一般用 L2-loss 或 smooth L1-loss 刻画；在嵌入向量的非监督学习中，则一般用余弦相似度刻画嵌入向量间的一致性；
\end{enumerate}
因此我们提出，应针对所用神经网络模型的任务使用相应的一致性指标，并以\textit{相同输入}下，全精度模型和量化模型\textit{最终输出}间的一致性，作为量化误差的衡量标准。也就是说，相较于模型训练或推理时某些中间层的局部误差，我们更关注模型最终输出间的不匹配度。

\paragraph{在模型预训练阶段即引入量化误差}
之前基于量化感知训练的工作，一般采取“预训练——量化感知微调”的两段式训练方法来减少神经网络的量化误差。然而，这类训练流程的代价是训练时间消耗几乎是传统训练的 $2\times$ 以上，且在量化感知微调阶段需要模型的完整训练数据集。例如在 LIQ 的 ImageNet 分类任务上，模型先在预训练阶段训练 120 轮，之后再在量化感知微调阶段训练 $60\sim 90$ 轮。如果考虑在量化感知微调阶段，模型计算图中包含的更多额外操作（例如量化、反量化、量化 STE 算子的梯度修改等），则在 ImageNet 分类任务上 LIQ 的总训练时长约是一般全精度模型的 $2\times$ 左右。与此同时，在实际应用场景中，模型在量化部署前一般很难接触到完整的训练数据集（例如模型的设计、训练和部署分别由不同的技术团队负责，各团队间数据不一定共享），这直接导致了两段式量化感知训练在实际场景中不一定可行。因此在 GQ-Nets 在能够接触完整训练数据的模型预训练阶段，即开始考虑模型量化运行的误差，并直接在模型参数优化时，尽可能消除该误差。

\paragraph{调和训练阶段多任务梯度}
对于较复杂的神经网络模型而言，其训练过程已包含来自不同任务损失函数的梯度——例如在用于实例分割的 Mask R-CNN 训练过程中，模型参数同时被最小化分类损失、检测锚框回归损失和分割掩模的三组梯度更新。在 GQ-Nets 的训练过程中，最小化量化误差会为模型参数生成另一组梯度。使用多组随训练过程频繁变化的梯度，优化同一组模型参数是较为困难的。因此，GQ-Nets 提出的在预训练阶段减小量化误差的挑战之一是，调和各损失函数的对应梯度，使模型能够收敛至同时最小化所有损失函数的解。
% ------------------------------------------------------------------------------
%    Contributions
% ------------------------------------------------------------------------------
\subsection{主要贡献}
\begin{enumerate}[1.]
  \item 本章提出了 GQ-Nets，一种使用量化神经网络与全精度神经网络间端到端不匹配度作为量化误差建模，并将该误差引入模型预训练阶段，从而直接得到量化友好的神经网络模型的训练方法；
  \item 本章深入分析了 GQ-Nets 在训练阶段的诸多特性，提出并验证了调和多任务训练不同梯度的多个方法；
  \item 本章在不同任务和大量真实数据集上验证了 GQ-Nets 的有效性，包括在 CIFAR、ImageNet 数据集上的图片分类任务，以及在 MS~COCO \improvement{在 mmdetectin 上补实验} 数据集上的多类别目标检测任务。
\end{enumerate}

% ==============================================================================
%  Modeling Quantization Errors
% ==============================================================================
\section{量化误差的建模}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{GQ_Nets/arch.pdf}
  \caption{GQ-Nets 的主要结构。其中，……}
  \label{img::gq_nets::arch}
\end{figure}

为了能在预训练阶段引入模型量化误差，GQ-Nets 在初始全精度神经网络模型的基础上，使用同一套模型参数\textit{并行地}构建一个量化神经网络模型，并将其在同一输入下的最终输出与全精度模型的输出相类比，作为量化误差的衡量指标。GQ-Nets 的主要结构如图~\ref{img::gq_nets::arch} 所示，其具体组成部分及运行流程如下：
\begin{enumerate}[1.]
  \item 初始 $L$ 层全精度神经网络模型 $f_{\mathcal{W}}$，其中 $\mathcal{W} = \{W_1, \ldots, W_L\}$ 表示模型的权重参数（下文统称为\textit{权重参数}），而 $W_i, i \in 1 \ldots L$ 分别表示模型第 $i$ 层的权重参数，均以浮点形式存储及运算；
  \item 由 $f_{\mathcal{W}}$ 构建而来的量化神经网络模型 $\hat{f}_{\mathcal{W}, \mathcal{Q}}$，其中 $\mathcal{Q}$ 表示量化模型在运行时所用的量化函数。 具体地，$\mathcal{Q} = \{Q^w_1, \ldots , Q^w_L, Q^a_1, \ldots , Q^a_L\}$ ，其中 $Q^w_i, i \in 1\ldots L$ 表示用于量化第 $i$ 层权重参数的量化函数，$Q^a_i, i \in 1\ldots L$ 表示用于量化第 $i$ 层模型激活的量化函数。注意在 GQ-Nets 中，每一个量化函数都分别被一组可学习参数 $\theta^w_i$ （用于参数量化函数 $Q^w_i$ ）或 $\theta^a_i$ （用于激活量化函数 $Q^a_i$ ）参数化，下文统称 $\Theta = \{\theta^w_1, \ldots , \theta^w_L, \theta^a_1, \ldots , \theta^a_L\}$ 为\textit{量化参数}。注意 $\hat{f}_{\mathcal{W}, \mathcal{Q}}$ 通过权重参数量化函数 $Q^w_{1\ldots L}$，与 $f_{\mathcal{W}}$ 共享一套权重参数 $\mathcal{W}$；
  \item GQ-Nets 模型各层在前向传播时，同时计算浮点和量化的中间激活值。具体地，在得到一经过预处理的、以浮点格式存储的小批量输入 $x_0$ 后，第 $i=1$ 层首先计算该层的浮点输出激活 $x_1 = x_0 \star_1 W_1$，其中 $\star_1$ 表示该层的对应操作（例如卷积、逐通道分离卷积或池化等）；之后将该层的输入 $x_0$、权重参数 $W_1$ 使用对应的量化函数及量化参数进行量化 $\hat{x}_0 = Q^a_1(x_0; \theta^a_1), \hat{W}_1 = Q^w_1(W_1; \theta^w_1)$，即可计算得到该层的量化输出激活 $\tilde{x}_1 = \hat{x}_0 \star_1 \hat{W}_1$。对于 $i\in 2\ldots L$ 层，浮点和量化前向计算不再共享输入，即分别使用 $i-1$ 层的浮点输出激活 $x_{i-1}$ 和量化输出激活 $\tilde{x}_{i-1}$ 计算本层的浮点输出 $x_i = x_{i-1} \star_i W_i$ 和量化输出 $\tilde{x}_i = Q^a_i(\tilde{x}_{i-1}; \theta^a_i) \star_i Q^w_i({W}_{i}; \theta^w_i)$ 。注意各层在执行量化前向计算时，由于其输入和权重参数均已被量化，故 $\tilde{x}_i$ 可完全由定点或整数数值逻辑计算得出；
  \item 在得到第 $L$ 层的浮点输出 $x_L$ 和量化输出 $\tilde{x}_L$ 后，分别计算反映模型在目标任务上准确度的原损失函数 $\mathcal{L}_f$ 及反映模型量化输出与浮点输出间相似程度的量化误差度量 $\mathcal{L}_q$，并由此计算模型训练的总体损失函数 $\mathcal{L}$。不失一般性地，本章中使用线性加权计算 $\mathcal{L}$，即
  \begin{align}
    \mathcal{L} = w_f \mathcal{L}_f + w_q \mathcal{L}_q \label{eq::gq_nets::total_loss}
  \end{align}
  其中 $w_f, w_q \in \mathbb{R}$ 分别表示模型训练过程对准确度和量化误差减少的偏重程度，并可以在训练过程中动态调整。对于分类任务，$\mathcal{L}_f$ 一般表示为模型最终浮点输出 $x_L$ 对应的概率分布与当前小批量输入标签 $y$ 间的交叉熵 $\mathcal{L}_f = \CE{\sigma(x_L)}{y}$，而 $\mathcal{L}_q$ 则表示为模型量化输出对应的概率分布相较于浮点输出对应的概率分布间的 KL--散度 $\mathcal{L}_q = \KL{\sigma(\tilde{x}_L)}{\sigma(x_L)}$，其中 $\sigma(\cdot)$ 表示 softmax 操作。通过优化~\eqref{eq::gq_nets::total_loss} 的第二项，模型的量化输出将尽可能接近浮点输出，即达到了在预训练时减小量化误差的目标。
\end{enumerate}

% ==============================================================================
%  Method
% ==============================================================================
\section{反向蒸馏训练}
TODO
% ------------------------------------------------------------------------------
%    Loss function
% ------------------------------------------------------------------------------
\subsection{损失函数设计}
TODO
% ------------------------------------------------------------------------------
%    Optimization
% ------------------------------------------------------------------------------
\subsection{损失函数优化}
TODO
% ------------------------------------------------------------------------------
%    Multi-domain BN
% ------------------------------------------------------------------------------
\subsection{多域批量正则化}
TODO

% ==============================================================================
%  Experiments
% ==============================================================================
\section{主要实验结果}
TODO

% ==============================================================================
%  Ablation study
% ==============================================================================
\section{对比实验及分析}
TODO

% ==============================================================================
%  Conclusion
% ==============================================================================
\section{结论}
TODO

